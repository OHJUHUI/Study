{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN3AII4UwMuGhGI4MzvrmuS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras import optimizers"],"metadata":{"id":"JwsLLfYd_sw-","executionInfo":{"status":"ok","timestamp":1689221569998,"user_tz":-540,"elapsed":2,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"5qqeB79j0Kh7","executionInfo":{"status":"ok","timestamp":1689221570499,"user_tz":-540,"elapsed":2,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip = True, width_shift_range = 0.1, height_shift_range=0.1)\n","train_generator = train_datagen.flow_from_directory(\"/content/train.csv\", target_size=(150, 150), batch_size=5, class_mode='binary')\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","test_generator = test_datagen.flow_from_directory(\"/content/test.csv\", target_size=(150, 150), batch_size=5, class_mode='binary')\n","\n","model = Sequential()\n","model.add(Conv2D(32,(3,3), input_shape=(150,150,3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Conv2D(32,(3,3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Conv2D(64,(3,3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Flatten())\n","model.add(Dense(64))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(1))\n","model.add(Activation('sigmaid'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"6zBESsX-6nVC","executionInfo":{"status":"error","timestamp":1689213056357,"user_tz":-540,"elapsed":1378,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}},"outputId":"5bd94aad-2b5a-4ebf-e7db-cfc17591d638"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NotADirectoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-49a5151a6f67>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizontal_flip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_shift_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_shift_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_datagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \"\"\"\n\u001b[0;32m-> 1648\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/train.csv'"]}]},{"cell_type":"code","source":["from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n","import numpy as np\n","\n","model = ResNet50(weights = 'imagenet')\n","img_path = '/content/KakaoTalk_20230713_100539518.jpg'\n","img = image.load_img(img_path, target_size = (224, 224)) # 영상 크기를 변경하고 적재한다.\n","x = image.img_to_array(img) # 영상을 넘파이 배열로 변환한다.\n","x = np.expand_dims(x, axis = 0) # 차원을 하나 늘인다. 배치 크기가 필요하다.\n","x = preprocess_input(x) # ResNet50이 요구하는 전처리를 한다.\n","preds = model.predict(x)\n","print('예측:', decode_predictions(preds, top = 3)[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lyoCBRVP7zZM","executionInfo":{"status":"ok","timestamp":1689214613891,"user_tz":-540,"elapsed":13436,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}},"outputId":"d85580ab-c2ab-4fcd-c717-d7d59f39f743"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 12s 12s/step\n","Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n","35363/35363 [==============================] - 0s 0us/step\n","예측: [('n02111889', 'Samoyed', 0.9553376), ('n02114548', 'white_wolf', 0.018170264), ('n02112018', 'Pomeranian', 0.009937643)]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"38drW5flgDTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lBVxF5YWgDVb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mEFjHhzNgDXH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lnuRdmpJgDZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGG16모델"],"metadata":{"id":"blNcGk3igDcx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras import optimizers"],"metadata":{"id":"7Kc7F57whH2G","executionInfo":{"status":"ok","timestamp":1689227786741,"user_tz":-540,"elapsed":259,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import *\n","\n","(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()"],"metadata":{"id":"9KpA1UqvCKA4","executionInfo":{"status":"ok","timestamp":1689227790840,"user_tz":-540,"elapsed":3698,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.utils import plot_model"],"metadata":{"id":"IbXD43zvf-i4","executionInfo":{"status":"ok","timestamp":1689227790841,"user_tz":-540,"elapsed":5,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from tensorflow_datasets.core.utils.lazy_imports_utils import tensorflow as tf\n","import tensorflow_datasets.public_api as tfds"],"metadata":{"id":"fsgi5Ae1lCKW","executionInfo":{"status":"ok","timestamp":1689227790841,"user_tz":-540,"elapsed":4,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["img = image.load_img(img_path, target_size = (224, 224))  # 크기변경 적재"],"metadata":{"id":"v_ga79F-jWTp","executionInfo":{"status":"ok","timestamp":1689227790841,"user_tz":-540,"elapsed":4,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import json\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch\n","import torchvision\n","from torchvision import models, transforms\n","\n","# *-- VGG model 읽어오기 --*\n","use_pretrained=True # 학습 된 파라미터 사용\n","net = models.vgg16(pretrained=use_pretrained)\n","net.eval()\n","# 모델 네트워크 구성 출력\n","print(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYLXRCBKww4e","executionInfo":{"status":"ok","timestamp":1689226813648,"user_tz":-540,"elapsed":19406,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}},"outputId":"0b48710e-e923-44ae-aeb8-3a5f65c33a02"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:08<00:00, 63.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["# 전처리 클래스\n","class BaseTransform() :\n","    def __init__(self , resize, mean, std) :\n","        self.base_transform = transforms.Compose([\n","            transforms.Resize(resize), # 짧은 변의 길이 기준으로 resize\n","            transforms.CenterCrop(resize), # 화상 중앙을 resize * resize 로 자름\n","            transforms.ToTensor(), # 토치 텐서로 변환\n","            transforms.Normalize(mean, std) # 색상 정보 표준화\n","        ])\n","\n","    def __call__(self, img) :\n","        return self.base_transform(img)"],"metadata":{"id":"qnRcAbRaw1N3","executionInfo":{"status":"ok","timestamp":1689226825834,"user_tz":-540,"elapsed":264,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["image_file_path = '/content/aa.jpg'\n","img = Image.open(image_file_path)"],"metadata":{"id":"fJBjtdw6w83P","executionInfo":{"status":"ok","timestamp":1689226964652,"user_tz":-540,"elapsed":385,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from keras.datasets import mnist\n","from keras.utils.np_utils import to_categorical\n","\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# reshaping X data: (n, 28, 28) => (n, 28, 28, 1)\n","X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n","X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n","# converting y data into categorical (one-hot encoding)\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","\n","print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6hwWVQF-0Vv0","executionInfo":{"status":"ok","timestamp":1689228938803,"user_tz":-540,"elapsed":1012,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}},"outputId":"41fc267a-0a51-4908-846b-8bacbafbf6e7"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n","(60000, 28, 28, 1) (10000, 28, 28, 1) (60000, 10) (10000, 10)\n"]}]},{"cell_type":"code","source":["from keras.layers import BatchNormalization, Dropout\n","def deep_cnn_advanced():\n","    model = Sequential()\n","\n","    model.add(Conv2D(input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3]), filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = (2,2)))\n","    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = (2,2)))\n","    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = (2,2)))\n","\n","    # prior layer should be flattend to be connected to dense layers\n","    model.add(Flatten())\n","    # dense layer with 50 neurons\n","    model.add(Dense(50, activation = 'relu', kernel_initializer='he_normal'))\n","    model.add(Dropout(0.5))\n","    # final layer with 10 neurons to classify the instances\n","    model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal'))\n","\n","    adam = optimizers.Adam(lr = 0.001)\n","    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n","\n","    return model\n","model = deep_cnn_advanced()\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ouXDLHpL6NJl","executionInfo":{"status":"ok","timestamp":1689229256191,"user_tz":-540,"elapsed":1078,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}},"outputId":"20215030-73de-428d-ab2c-f300206394bb"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 28, 28, 50)        500       \n","                                                                 \n"," batch_normalization (BatchN  (None, 28, 28, 50)       200       \n"," ormalization)                                                   \n","                                                                 \n"," activation (Activation)     (None, 28, 28, 50)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 28, 28, 50)        22550     \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 28, 28, 50)       200       \n"," hNormalization)                                                 \n","                                                                 \n"," activation_1 (Activation)   (None, 28, 28, 50)        0         \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 14, 14, 50)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 14, 14, 50)        22550     \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 14, 14, 50)       200       \n"," hNormalization)                                                 \n","                                                                 \n"," activation_2 (Activation)   (None, 14, 14, 50)        0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 14, 14, 50)        22550     \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 14, 14, 50)       200       \n"," hNormalization)                                                 \n","                                                                 \n"," activation_3 (Activation)   (None, 14, 14, 50)        0         \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 7, 7, 50)         0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 7, 7, 50)          22550     \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 7, 7, 50)         200       \n"," hNormalization)                                                 \n","                                                                 \n"," activation_4 (Activation)   (None, 7, 7, 50)          0         \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 7, 7, 50)          22550     \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 7, 7, 50)         200       \n"," hNormalization)                                                 \n","                                                                 \n"," activation_5 (Activation)   (None, 7, 7, 50)          0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 3, 3, 50)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 450)               0         \n","                                                                 \n"," dense (Dense)               (None, 50)                22550     \n","                                                                 \n"," dropout (Dropout)           (None, 50)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                510       \n","                                                                 \n","=================================================================\n","Total params: 137,510\n","Trainable params: 136,910\n","Non-trainable params: 600\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from keras.layers import BatchNormalization, Dropout\n","def deep_cnn_advanced():\n","    model = Sequential()\n","\n","    model.add(Conv2D(input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3]), filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = (2,2)))\n","    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = (2,2)))\n","    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n","    model.add(BatchNormalization())\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size = (2,2)))\n","\n","    # prior layer should be flattend to be connected to dense layers\n","    model.add(Flatten())\n","    # dense layer with 50 neurons\n","    model.add(Dense(50, activation = 'relu', kernel_initializer='he_normal'))\n","    model.add(Dropout(0.5))\n","    # final layer with 10 neurons to classify the instances\n","    model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal'))\n","\n","    adam = optimizers.Adam(lr = 0.001)\n","    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n","\n","    return model\n","model = deep_cnn_advanced()\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pVwSc3Xr6OA3","executionInfo":{"status":"ok","timestamp":1689229807945,"user_tz":-540,"elapsed":1649,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}},"outputId":"27e78142-e76c-4d05-ef58-574d20c6243f"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_6 (Conv2D)           (None, 28, 28, 50)        500       \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 28, 28, 50)       200       \n"," hNormalization)                                                 \n","                                                                 \n"," activation_6 (Activation)   (None, 28, 28, 50)        0         \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 28, 28, 50)        22550     \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 28, 28, 50)       200       \n"," hNormalization)                                                 \n","                                                                 \n"," activation_7 (Activation)   (None, 28, 28, 50)        0         \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 14, 14, 50)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 14, 14, 50)        22550     \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 14, 14, 50)       200       \n"," hNormalization)                                                 \n","                                                                 \n"," activation_8 (Activation)   (None, 14, 14, 50)        0         \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 14, 14, 50)        22550     \n","                                                                 \n"," batch_normalization_9 (Batc  (None, 14, 14, 50)       200       \n"," hNormalization)                                                 \n","                                                                 \n"," activation_9 (Activation)   (None, 14, 14, 50)        0         \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 7, 7, 50)         0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 7, 7, 50)          22550     \n","                                                                 \n"," batch_normalization_10 (Bat  (None, 7, 7, 50)         200       \n"," chNormalization)                                                \n","                                                                 \n"," activation_10 (Activation)  (None, 7, 7, 50)          0         \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 7, 7, 50)          22550     \n","                                                                 \n"," batch_normalization_11 (Bat  (None, 7, 7, 50)         200       \n"," chNormalization)                                                \n","                                                                 \n"," activation_11 (Activation)  (None, 7, 7, 50)          0         \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 3, 3, 50)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_1 (Flatten)         (None, 450)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 50)                22550     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 50)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                510       \n","                                                                 \n","=================================================================\n","Total params: 137,510\n","Trainable params: 136,910\n","Non-trainable params: 600\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"QRMmmOV66SgA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WcYAZGPiBKEC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jkccp7biBKGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CrSkfKWRBKJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QZJM5KQgFE0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hPu0MpDoBKL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jKnMFpGIBcC4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","from keras import optimizers\n","from keras.models import Sequential\n","from keras.layers import Dropout, Flatten, Dense\n","from keras.models import Model\n","from keras import models\n","from keras import layers\n","from keras import optimizers\n","import keras.backend as K"],"metadata":{"id":"HhlINE6dBKN8","executionInfo":{"status":"ok","timestamp":1689231350619,"user_tz":-540,"elapsed":325,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["from keras.applications import vgg16\n","\n","# VGG16 모델을 불러오기\n","model = vgg16.VGG16()\n","\n","# 모델의 모양을 보여준다.\n","model.summary()\n","\n","\n","files = [\n","    'dental_image/test/healthy/1.jpg',\n","    'dental_image/test/decayed/101.jpg',\n","    'dental_image/test/cured/301.jpg'\n","        ]\n","\n","\n","for file in files :\n","  predict_vgg16(model, file)"],"metadata":{"id":"Pvf09FyJCtG7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","train_dir = 'dental_image/train'\n","validation_dir = 'dental_image/test'\n","batch_size = 32\n","image_size = 224\n","\n","# 학습에 사용될 이미지 데이터 생성기\n","train_datagen = ImageDataGenerator(\n","      rotation_range=180, # 회전 쵀대 20도\n","      width_shift_range=0.2, # 좌우 이동\n","      height_shift_range=0.2, # 상하 이동\n","      horizontal_flip=True, # 좌우 반전\n","      vertical_flip=True, # 상하 반전\n","      )\n","\n","# 검증에 사용될 이미지 데이터 생성기\n","validation_datagen = ImageDataGenerator()\n","\n","\n","# 학습에 사용될 데이터 생성기\n","train_generator = train_datagen.flow_from_directory(\n","        train_dir,\n","        target_size=(image_size, image_size),\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        shuffle=True)\n","\n","# 검증에 사용될 데이터 생성기\n","validation_generator = validation_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(image_size, image_size),\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        shuffle=False)\n","\n","class_num=len(train_generator.class_indices)\n","\n","custom_labels = list(validation_generator.class_indices.keys())"],"metadata":{"id":"mThYStTWCqlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["K.clear_session() # 새로운 세션으로 시작\n","\n","from keras.applications import VGG16\n","# 모델 불러오기\n","conv_layers = VGG16('', weights='imagenet', include_top=False, input_shape=(image_size=3))\n","conv_layers.summary()\n","\n","# Convolution Layer를 학습되지 않도록 고정\n","for layer in conv_layers.layers:\n","    layer.trainable = False\n","\n","\n","# 새로운 모델 생성하기\n","model = models.Sequential()\n","\n","# VGG16모델의 Convolution Layer를 추가\n","model.add(conv_layers)\n","\n","# 모델의 Fully Connected 부분을 재구성\n","model.add(layers.Flatten())\n","model.add(layers.Dense(1024, activation='relu'))\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(class_num, activation='softmax'))\n","\n","# 모델\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":264},"id":"ra_aF2-iCNhO","executionInfo":{"status":"error","timestamp":1689231398406,"user_tz":-540,"elapsed":361,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}},"outputId":"8144f424-fbd9-419f-bb62-c157a14866e1"},"execution_count":33,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-890a23b478bb>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 모델 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mconv_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mconv_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'image_size' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"F-ky5AWRCZL9"},"execution_count":null,"outputs":[]}]}