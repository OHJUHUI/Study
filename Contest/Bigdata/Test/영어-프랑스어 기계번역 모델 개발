{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689915723579,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"e0Pecv2dtG_c"},"outputs":[],"source":["# seq2seq를 이용한 기계 번역기 제작"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3446,"status":"ok","timestamp":1689915727023,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"JNoR-ydKWh6L"},"outputs":[],"source":["import os\n","import re\n","import shutil\n","import zipfile\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import unicodedata\n","import urllib3\n","from tensorflow.keras.layers import Embedding, GRU, Dense\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689915727023,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"uip2poXIYElD"},"outputs":[],"source":["num_samples = 100000"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1689915727023,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"6XzFllxhZjHa"},"outputs":[],"source":["# 전처리 과정\n","def to_ascii(s):\n","# 프랑스어 악센트 제거\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","                   if unicodedata.category(c) != 'Mn')\n","\n","def preprocess_sentence(sent):\n","# 단어와 구두점 사이에 공백 추가\n","  sent = to_ascii(sent.lower())\n","\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","# 선언 기호를 제외한 기호는 공백으로 변환\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","# 다수의 공백 -> 하나의 공백\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689915727024,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"MqQL0_IpZnyM","outputId":"f95c451f-bb23-4ffc-a8bf-87760325aca4"},"outputs":[{"output_type":"stream","name":"stdout","text":["전처리 전 영어 문장 : Have you had dinner?\n","전처리 후 영어 문장 : have you had dinner ?\n","전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n","전처리 후 프랑스어 문장 : avez vous deja dine ?\n"]}],"source":["# 전처리 함수 테스트\n","en_sent = u\"Have you had dinner?\"\n","fr_sent = u\"Avez-vous déjà diné?\"\n","\n","print('전처리 전 영어 문장 :', en_sent)\n","print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n","print('전처리 전 프랑스어 문장 :', fr_sent)\n","print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1689915727024,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"YAe4u72bZ4wW"},"outputs":[],"source":["# 레이블에 해당되는 출력 시퀀스 분리 후 저장\n","# 입력시퀀스 <sos> (-> 시작을 의미하는 토큰)\n","# 출력시퀀스 <eos> (-> 종료를 의미하는 토큰)\n","\n","def load_preprocessed_data():\n","  encoder_input, decoder_input, decoder_target = [], [], []\n","\n","  with open(\"fra.txt\", \"r\") as lines:\n","    for i, line in enumerate(lines):\n","      # source 데이터와 target 데이터 분리\n","      src_line, tar_line, _ = line.strip().split('\\t')\n","\n","      # source 데이터 전처리\n","      src_line = [w for w in preprocess_sentence(src_line).split()]\n","\n","      # target 데이터 전처리\n","      tar_line = preprocess_sentence(tar_line)\n","      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","\n","      if i == num_samples - 1:\n","        break\n","\n","  return encoder_input, decoder_input, decoder_target"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5237,"status":"ok","timestamp":1689915732258,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"QCJZvIQaaI09","outputId":"474ba782-946d-489f-bf19-2f6322a02072"},"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n","디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n","디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"]}],"source":["# 인코더의 입력, 디코더의 입력, 디코더의 레이블 상위 5개 샘플 출력\n","sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n","print('인코더의 입력 :',sents_en_in[:5])\n","print('디코더의 입력 :',sents_fra_in[:5])\n","print('디코더의 레이블 :',sents_fra_out[:5])"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3226,"status":"ok","timestamp":1689915735475,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"tBHhrdSBaO6J"},"outputs":[],"source":["# 기계 번역 (Machine Translation)을 위한 데이터 전처리를 수행\n","\n","# 영어 데이터 Tokenizer 객체를 생성(lower=False는 영어 문장을 소문자로 변환하지 않음)\n","tokenizer_en = Tokenizer(filters=\"\", lower=False)\n","# sents_en_in을 토큰화하여 단어 사전을 구축\n","tokenizer_en.fit_on_texts(sents_en_in)\n","# 토큰화된 영어 문장들을 정수 시퀀스로 변환\n","encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n","# encoder_input에 대해 패딩을 수행\n","encoder_input = pad_sequences(encoder_input, padding=\"post\")\n","\n","# 프랑스 데이터 새로운 Tokenizer 객체를 생성\n","tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n","# sents_fra_in을 토큰화하여 단어 사전을 구축\n","tokenizer_fra.fit_on_texts(sents_fra_in)\n","# sents_fra_out도 같은 단어 사전을 이용하여 구축\n","tokenizer_fra.fit_on_texts(sents_fra_out)\n","\n","# 토큰화된 프랑스어 입력 문장들을 정수 시퀀스로 변환\n","decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n","# decoder_input에 대해 패딩을 수행\n","decoder_input = pad_sequences(decoder_input, padding=\"post\")\n","\n","# 토큰화된 프랑스어 출력 문장들을 정수 시퀀스로 변환\n","decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n","# decoder_target에 대해 패딩을 수행\n","decoder_target = pad_sequences(decoder_target, padding=\"post\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1689915735475,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"jB8-QjQRag0u","outputId":"e80e9ac9-575f-4bfe-fbc4-f36cd6f1a1c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력의 크기(shape) : (100000, 9)\n","디코더의 입력의 크기(shape) : (100000, 17)\n","디코더의 레이블의 크기(shape) : (100000, 17)\n"]}],"source":["# 데이터 크기 확인\n","print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n","print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n","print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1689915735476,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"7DvZy-U-ai0B","outputId":"4d0208bc-5bc9-499d-86dc-84a22d36dfbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["영어 단어 집합의 크기 : 8656, 프랑스어 단어 집합의 크기 : 14584\n"]}],"source":["src_vocab_size = len(tokenizer_en.word_index) + 1\n","tar_vocab_size = len(tokenizer_fra.word_index) + 1\n","print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689915735476,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"1UH7gr6TanG2"},"outputs":[],"source":["# tokenizer_en 객체를 사용하여 영어 단어를 정수 인덱스로 매핑하는 딕셔너리인 src_to_index를 생성\n","src_to_index = tokenizer_en.word_index\n","# tokenizer_en 객체를 사용하여 정수 인덱스를 영어 단어로 매핑하는 딕셔너리인 index_to_src를 생성\n","index_to_src = tokenizer_en.index_word\n","\n","# tokenizer_fra 객체를 사용하여 프랑스어 단어를 정수 인덱스로 매핑하는 딕셔너리인 tar_to_index를 생성\n","tar_to_index = tokenizer_fra.word_index\n","# tokenizer_fra 객체를 사용하여 정수 인덱스를 프랑스어 단어로 매핑하는 딕셔너리인 index_to_tar를 생성\n","index_to_tar = tokenizer_fra.index_word"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689915735476,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"-wMqt5zVas6c","outputId":"ba06893a-09f6-47a2-a263-f8ab7be475ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["랜덤 시퀀스 :  [30045 29271 86720 ... 55821 74049 39302]\n"]}],"source":["# encoder_input의 행의 개수(데이터 샘플의 수)를 기반으로 0부터 시작하여 연속적인 정수로 채워진 넘파이 배열 indices를 생성\n","indices = np.arange(encoder_input.shape[0])\n","# np.random.shuffle 함수를 사용하여 indices 배열의 원소를 임의로 섞기\n","np.random.shuffle(indices)\n","print('랜덤 시퀀스 : ', indices)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1689915736135,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"ZGhVrUtrbX3K"},"outputs":[],"source":["# encoder_input의 데이터가 랜덤한 순서로 재배열\n","encoder_input = encoder_input[indices]\n","# decoder_input 배열도 indices 배열의 순서에 맞추어 재정렬\n","decoder_input = decoder_input[indices]\n","# decoder_target 배열도 indices 배열의 순서에 맞추어 재정렬\n","decoder_target = decoder_target[indices]"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1689915736136,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"K17xT5RLcAtz","outputId":"ab67024e-9082-4264-ed62-969d0490bdce"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 70, 232,  11, 864, 132,   1,   0,   0,   0], dtype=int32)"]},"metadata":{},"execution_count":14}],"source":["encoder_input[5000]"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1689915736136,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"Lb9mnaKlcCG3","outputId":"5f2697cb-495b-4cb5-d2a7-28aca4562dbc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   2, 1841,   19,  943,   38,   12,   45,  113,    1,    0,    0,\n","          0,    0,    0,    0,    0,    0], dtype=int32)"]},"metadata":{},"execution_count":15}],"source":["decoder_input[5000]"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1689915736137,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"A7_zLemccEc6","outputId":"27012ea8-87bf-4fcd-fdfa-25978cea26bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1841,   19,  943,   38,   12,   45,  113,    1,    3,    0,    0,\n","          0,    0,    0,    0,    0,    0], dtype=int32)"]},"metadata":{},"execution_count":16}],"source":["decoder_target[5000]"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1689915736137,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"GZL1ei6lcGKl","outputId":"85423883-e1c2-49ed-8962-0bb5e3ca53e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["검증 데이터의 개수 : 10000\n"]}],"source":["# 훈련 데이터의 10%를 테스트 데이터로 분리\n","n_of_val = int(100000*0.1)\n","print('검증 데이터의 개수 :',n_of_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvmDinzocH63"},"outputs":[],"source":[" # 검증 데이터의 개수 10,000개의 데이터를 테스트 데이터로 사용\n","encoder_input_train = encoder_input[:-n_of_val]\n","decoder_input_train = decoder_input[:-n_of_val]\n","decoder_target_train = decoder_target[:-n_of_val]\n","\n","encoder_input_test = encoder_input[-n_of_val:]\n","decoder_input_test = decoder_input[-n_of_val:]\n","decoder_target_test = decoder_target[-n_of_val:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689837893687,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"guepvtFpcJuS","outputId":"5180ffb0-ee03-4ec2-e25b-0681a973ff6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["훈련 source 데이터의 크기 : (108000, 10)\n","훈련 target 데이터의 크기 : (108000, 19)\n","훈련 target 레이블의 크기 : (108000, 19)\n","테스트 source 데이터의 크기 : (12000, 10)\n","테스트 target 데이터의 크기 : (12000, 19)\n","테스트 target 레이블의 크기 : (12000, 19)\n"]}],"source":["# 훈련데이터와 테스트 데이터의 크기 출력\n","print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n","print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n","print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n","print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n","print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n","print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpsNrfe_cMAF"},"outputs":[],"source":["# 기계 번역기 만들기\n","# 임베딩 벡터의 차원과 LSTM의 은닉 상태의 크기를 64로 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQVBY1qJ9aKf"},"outputs":[],"source":["from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n","from tensorflow.keras.models import Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUjrLjo49ayT"},"outputs":[],"source":["embedding_dim = 64\n","hidden_units = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aG_IEPk-a-0"},"outputs":[],"source":["encoder_inputs = Input(shape=(None,))\n","# Embedding 레이어를 생성하여 입력 시퀀스를 임베딩 벡터로 변환\n","enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs)\n","# Masking 레이어를 생성하여 입력 시퀀스에서 패딩을 마스킹\n","enc_masking = Masking(mask_value=0.0)(enc_emb)\n","encoder_lstm = LSTM(hidden_units, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n","encoder_states = [state_h, state_c]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gkSfDsG-chp"},"outputs":[],"source":["decoder_inputs = Input(shape=(None,))\n","dec_emb_layer = Embedding(tar_vocab_size, hidden_units)\n","dec_emb = dec_emb_layer(decoder_inputs)\n","dec_masking = Masking(mask_value=0.0)(dec_emb)\n","\n","decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n","\n","decoder_outputs, _, _ = decoder_lstm(dec_masking,\n","                                     initial_state=encoder_states)\n","\n","decoder_dense = Dense(tar_vocab_size, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"H5hL7xI_-f0X","outputId":"9bc30c52-eee0-4ea1-e47b-6f42a5a16547"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","844/844 [==============================] - 82s 86ms/step - loss: 2.4524 - acc: 0.6635 - val_loss: 1.7593 - val_acc: 0.7275\n","Epoch 2/50\n","844/844 [==============================] - 51s 60ms/step - loss: 1.6085 - acc: 0.7424 - val_loss: 1.4662 - val_acc: 0.7663\n","Epoch 3/50\n","844/844 [==============================] - 48s 57ms/step - loss: 1.3576 - acc: 0.7786 - val_loss: 1.2850 - val_acc: 0.7899\n","Epoch 4/50\n","844/844 [==============================] - 48s 56ms/step - loss: 1.2017 - acc: 0.7987 - val_loss: 1.1610 - val_acc: 0.8045\n","Epoch 5/50\n","844/844 [==============================] - 44s 52ms/step - loss: 1.0820 - acc: 0.8123 - val_loss: 1.0654 - val_acc: 0.8170\n","Epoch 6/50\n","844/844 [==============================] - 47s 56ms/step - loss: 0.9859 - acc: 0.8241 - val_loss: 0.9935 - val_acc: 0.8252\n","Epoch 7/50\n","844/844 [==============================] - 47s 56ms/step - loss: 0.9090 - acc: 0.8331 - val_loss: 0.9347 - val_acc: 0.8321\n","Epoch 8/50\n","844/844 [==============================] - 45s 54ms/step - loss: 0.8437 - acc: 0.8408 - val_loss: 0.8873 - val_acc: 0.8382\n","Epoch 9/50\n","844/844 [==============================] - 45s 53ms/step - loss: 0.7869 - acc: 0.8477 - val_loss: 0.8470 - val_acc: 0.8433\n","Epoch 10/50\n","844/844 [==============================] - 45s 54ms/step - loss: 0.7368 - acc: 0.8541 - val_loss: 0.8135 - val_acc: 0.8478\n","Epoch 11/50\n","844/844 [==============================] - 45s 53ms/step - loss: 0.6923 - acc: 0.8600 - val_loss: 0.7842 - val_acc: 0.8519\n","Epoch 12/50\n","844/844 [==============================] - 47s 55ms/step - loss: 0.6522 - acc: 0.8654 - val_loss: 0.7595 - val_acc: 0.8552\n","Epoch 13/50\n","844/844 [==============================] - 47s 55ms/step - loss: 0.6165 - acc: 0.8704 - val_loss: 0.7381 - val_acc: 0.8583\n","Epoch 14/50\n","844/844 [==============================] - 47s 56ms/step - loss: 0.5846 - acc: 0.8750 - val_loss: 0.7183 - val_acc: 0.8612\n","Epoch 15/50\n","844/844 [==============================] - 46s 54ms/step - loss: 0.5563 - acc: 0.8790 - val_loss: 0.7041 - val_acc: 0.8635\n","Epoch 16/50\n","844/844 [==============================] - 45s 53ms/step - loss: 0.5305 - acc: 0.8832 - val_loss: 0.6898 - val_acc: 0.8655\n","Epoch 17/50\n","844/844 [==============================] - 43s 51ms/step - loss: 0.5072 - acc: 0.8866 - val_loss: 0.6771 - val_acc: 0.8676\n","Epoch 18/50\n","844/844 [==============================] - 43s 51ms/step - loss: 0.4860 - acc: 0.8899 - val_loss: 0.6671 - val_acc: 0.8688\n","Epoch 19/50\n","844/844 [==============================] - 41s 49ms/step - loss: 0.4666 - acc: 0.8932 - val_loss: 0.6590 - val_acc: 0.8710\n","Epoch 20/50\n","844/844 [==============================] - 40s 48ms/step - loss: 0.4489 - acc: 0.8962 - val_loss: 0.6515 - val_acc: 0.8715\n","Epoch 21/50\n","844/844 [==============================] - 43s 51ms/step - loss: 0.4326 - acc: 0.8987 - val_loss: 0.6442 - val_acc: 0.8732\n","Epoch 22/50\n","844/844 [==============================] - 40s 48ms/step - loss: 0.4175 - acc: 0.9017 - val_loss: 0.6367 - val_acc: 0.8743\n","Epoch 23/50\n","844/844 [==============================] - 42s 50ms/step - loss: 0.4035 - acc: 0.9040 - val_loss: 0.6330 - val_acc: 0.8752\n","Epoch 24/50\n","844/844 [==============================] - 43s 51ms/step - loss: 0.3906 - acc: 0.9064 - val_loss: 0.6292 - val_acc: 0.8762\n","Epoch 25/50\n","844/844 [==============================] - 42s 50ms/step - loss: 0.3787 - acc: 0.9085 - val_loss: 0.6253 - val_acc: 0.8770\n","Epoch 26/50\n","844/844 [==============================] - 43s 51ms/step - loss: 0.3675 - acc: 0.9108 - val_loss: 0.6231 - val_acc: 0.8773\n","Epoch 27/50\n","844/844 [==============================] - 43s 50ms/step - loss: 0.3572 - acc: 0.9126 - val_loss: 0.6223 - val_acc: 0.8777\n","Epoch 28/50\n","844/844 [==============================] - 40s 47ms/step - loss: 0.3475 - acc: 0.9145 - val_loss: 0.6184 - val_acc: 0.8781\n","Epoch 29/50\n","844/844 [==============================] - 42s 50ms/step - loss: 0.3385 - acc: 0.9163 - val_loss: 0.6163 - val_acc: 0.8794\n","Epoch 30/50\n","844/844 [==============================] - 44s 52ms/step - loss: 0.3301 - acc: 0.9178 - val_loss: 0.6145 - val_acc: 0.8799\n","Epoch 31/50\n","844/844 [==============================] - 43s 51ms/step - loss: 0.3224 - acc: 0.9195 - val_loss: 0.6126 - val_acc: 0.8805\n","Epoch 32/50\n","844/844 [==============================] - 43s 50ms/step - loss: 0.3148 - acc: 0.9208 - val_loss: 0.6125 - val_acc: 0.8804\n","Epoch 33/50\n","844/844 [==============================] - 42s 50ms/step - loss: 0.3080 - acc: 0.9223 - val_loss: 0.6118 - val_acc: 0.8809\n","Epoch 34/50\n","844/844 [==============================] - 43s 50ms/step - loss: 0.3013 - acc: 0.9235 - val_loss: 0.6119 - val_acc: 0.8809\n","Epoch 35/50\n","844/844 [==============================] - 43s 50ms/step - loss: 0.2953 - acc: 0.9247 - val_loss: 0.6111 - val_acc: 0.8814\n","Epoch 36/50\n","844/844 [==============================] - 42s 50ms/step - loss: 0.2894 - acc: 0.9259 - val_loss: 0.6108 - val_acc: 0.8817\n","Epoch 37/50\n","844/844 [==============================] - 43s 51ms/step - loss: 0.2838 - acc: 0.9269 - val_loss: 0.6111 - val_acc: 0.8819\n","Epoch 38/50\n","844/844 [==============================] - 42s 50ms/step - loss: 0.2788 - acc: 0.9281 - val_loss: 0.6126 - val_acc: 0.8817\n","Epoch 39/50\n","844/844 [==============================] - 43s 51ms/step - loss: 0.2737 - acc: 0.9290 - val_loss: 0.6124 - val_acc: 0.8820\n","Epoch 40/50\n","844/844 [==============================] - 44s 52ms/step - loss: 0.2688 - acc: 0.9299 - val_loss: 0.6122 - val_acc: 0.8825\n","Epoch 41/50\n","844/844 [==============================] - 41s 48ms/step - loss: 0.2645 - acc: 0.9308 - val_loss: 0.6139 - val_acc: 0.8826\n","Epoch 42/50\n","844/844 [==============================] - 40s 47ms/step - loss: 0.2603 - acc: 0.9315 - val_loss: 0.6160 - val_acc: 0.8822\n","Epoch 43/50\n","844/844 [==============================] - 42s 50ms/step - loss: 0.2561 - acc: 0.9325 - val_loss: 0.6155 - val_acc: 0.8826\n","Epoch 44/50\n","844/844 [==============================] - 42s 50ms/step - loss: 0.2524 - acc: 0.9332 - val_loss: 0.6165 - val_acc: 0.8827\n","Epoch 45/50\n","844/844 [==============================] - 43s 50ms/step - loss: 0.2485 - acc: 0.9340 - val_loss: 0.6189 - val_acc: 0.8828\n","Epoch 46/50\n","844/844 [==============================] - 43s 51ms/step - loss: 0.2449 - acc: 0.9347 - val_loss: 0.6184 - val_acc: 0.8833\n","Epoch 47/50\n","844/844 [==============================] - 42s 50ms/step - loss: 0.2417 - acc: 0.9354 - val_loss: 0.6198 - val_acc: 0.8832\n","Epoch 48/50\n","844/844 [==============================] - 43s 50ms/step - loss: 0.2382 - acc: 0.9359 - val_loss: 0.6222 - val_acc: 0.8830\n","Epoch 49/50\n","844/844 [==============================] - 43s 51ms/step - loss: 0.2351 - acc: 0.9367 - val_loss: 0.6228 - val_acc: 0.8834\n","Epoch 50/50\n","844/844 [==============================] - 44s 52ms/step - loss: 0.2322 - acc: 0.9372 - val_loss: 0.6257 - val_acc: 0.8828\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7a61b8c60af0>"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n","          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n","          batch_size=128, epochs=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"e-JcfeFH-mJe"},"outputs":[],"source":["# Seq2Seq (Sequence-to-Sequence) 기계 번역 모델의 인코더와 디코더 부분을 정의하고, 모델을 생성\n","\n","# 인토더 모델 정의\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(hidden_units,))\n","decoder_state_input_c = Input(shape=(hidden_units,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n","decoder_states2 = [state_h2, state_c2]\n","\n","decoder_outputs2 = decoder_dense(decoder_outputs2)\n","\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs2] + decoder_states2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"d4s4zq2xAHkd"},"outputs":[],"source":["def decode_sequence(inpur_seq):\n","  states_value = encoder_model.predict(input_seq)\n","\n","  target_seq = np.zeros((1, 1))\n","  target_seq[0, 0] = tar_to_index['<sos>']\n","\n","  stop_condition = False\n","  decoded_sentence = ''\n","\n","  while not stop_condition:\n","    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_char = index_to_tar[sampled_token_index]\n","\n","    decoded_sentence += \" \" + sampled_char\n","\n","    if (sampled_char == '<eos>' or len(decoded_sentence) > 50 ) :\n","      stop_condition = True\n","\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = sampled_token_index\n","\n","    states_value = [h, c]\n","\n","  return decoded_sentence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UR17Drxebna5"},"outputs":[],"source":["# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_src(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0):\n","      sentence = sentence + index_to_src[encoded_word] + ' '\n","  return sentence\n","\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_tar(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n","      sentence = sentence + index_to_tar[encoded_word] + ' '\n","  return sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K5x8QYoAcgLE","outputId":"4cff66c4-46ed-42ed-f021-015a04be2d1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n","1/1 [==============================] - 0s 334ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","입력문장 : pigs can t fly . \n","정답문장 : les cochons ne volent pas . \n","번역문장 : les cochons ne volent pas . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","입력문장 : don t worry about this . \n","정답문장 : ne t inquiete pas a ce sujet . \n","번역문장 : ne t inquiete pas pour ca . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","입력문장 : don t make any noise . \n","정답문장 : ne fais pas de bruit . \n","번역문장 : ne fais pas de bruit . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","입력문장 : are you as tall as me ? \n","정답문장 : etes vous aussi grand que moi ? \n","번역문장 : etes vous aussi grand que moi ? \n","--------------------------------------------------\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 20ms/step\n","입력문장 : i think that s undeniable . \n","정답문장 : je pense que c est indeniable . \n","번역문장 : je pense que c est indeniable . \n","--------------------------------------------------\n"]}],"source":["# 훈련 데이터에 대해 임의 선택한 인덱스의 샘플 출력\n","for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_train[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print(\"-\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vAh29eN2qz6H","outputId":"957255e4-b2d3-45ab-d396-1f4f73b4104b"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","입력문장 : how do you know what i had ? \n","정답문장 : comment savez vous ce que j avais ? \n","번역문장 : comment sais tu ce que j ai fait ? \n","--------------------------------------------------\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 21ms/step\n","입력문장 : my horse is black . \n","정답문장 : mon cheval est noir . \n","번역문장 : mon cheval est noir a l ecole . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 18ms/step\n","입력문장 : don t use this faucet . \n","정답문장 : n utilise pas ce robinet . \n","번역문장 : n utilisez pas ce robinet . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","입력문장 : i m coming right away . \n","정답문장 : j arrive immediatement . \n","번역문장 : j arrive juste . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 30ms/step\n","입력문장 : i heard this movie is good . \n","정답문장 : j ai entendu dire que ce film est bon . \n","번역문장 : j ai entendu ce que tom a du mieux . \n","--------------------------------------------------\n"]}],"source":["# 테스트 데이터에서 임의 선택한 인덱스의 샘플 결과 출력\n","for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_test[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print(\"-\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90761,"status":"ok","timestamp":1689837985314,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"n5gEgUjlKwUJ","outputId":"eba9b2de-f7e0-4ffd-d046-f9999187e53a"},"outputs":[{"name":"stdout","output_type":"stream","text":["3375/3375 [==============================] - 61s 17ms/step - loss: nan - acc: 0.6963\n","학습 데이터 정확도: 0.6962583661079407\n","375/375 [==============================] - 7s 18ms/step - loss: nan - acc: 0.6963\n","검증 데이터 정확도: 0.6962778568267822\n"]}],"source":["# 학습 데이터에 대한 정확도 계산\n","train_loss, train_accuracy = model.evaluate(x=[encoder_input_train, decoder_input_train], y=decoder_target_train)\n","print(\"학습 데이터 정확도:\", train_accuracy)\n","\n","# 검증 데이터에 대한 정확도 계산\n","test_loss, test_accuracy = model.evaluate(x=[encoder_input_test, decoder_input_test], y=decoder_target_test)\n","print(\"검증 데이터 정확도:\", test_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ufhqn57sIeG1"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMifwxBMWrbpMOlKm/3uKje"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}