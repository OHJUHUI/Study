{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mT2lzPXS4YMX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일\n",
        "path = 'fra.txt'"
      ],
      "metadata": {
        "id": "xIlD76gA_drI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#유니코드 -> 아스키코드로\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD',s)\n",
        "    if unicodedata.category(c) != 'Mn')"
      ],
      "metadata": {
        "id": "iGo_Aw_E6PP-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower())\n",
        "\n",
        "  #단어와 . 사이 공백\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "\n",
        "  #a-z, A-Z, ., ?, !, ,을 제외하고는 모두 공백으로 변환\n",
        "  w = re.sub(r\"[^a-zA-Z?.!]+\", r\" \", w)\n",
        "\n",
        "  w = re.sub(r\"\\s+\", \" \", w)\n",
        "  return w"
      ],
      "metadata": {
        "id": "KGjdJMor7B9Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#전처리 테스트\n",
        "eng = \"May I borrow this book?\"\n",
        "fra = \"Puis-je emprunter ce livre?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', eng)\n",
        "print('전처리 후 영어 문장 :', preprocess_sentence(eng))\n",
        "print('\\n전처리 전 불어 문장 :', fra)\n",
        "print('전처리 후 불어 문장 :', preprocess_sentence(fra))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANwfgeX78wsR",
        "outputId": "49db256c-8fa1-43ec-f199-f3e2d6f1d360"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 전 영어 문장 : May I borrow this book?\n",
            "전처리 후 영어 문장 : may i borrow this book ? \n",
            "\n",
            "전처리 전 불어 문장 : Puis-je emprunter ce livre?\n",
            "전처리 후 불어 문장 : puis je emprunter ce livre ? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#사용할 샘플 수\n",
        "num_samples = 100000"
      ],
      "metadata": {
        "id": "8xNUAKMM65X0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  #\"fra.txt\" 파일을 읽기 모드로 열고 순회\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # '_' 필요하지 않은 정보를 저장하지 않기 위해사용\n",
        "      source_line, target_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      source_line = [w for w in preprocess_sentence(source_line).split()]\n",
        "\n",
        "      target_line = preprocess_sentence(target_line)\n",
        "      target_line_in = [w for w in (\"<sos> \" + target_line).split()]\n",
        "      target_line_out = [w for w in (target_line + \" <eos>\").split()]\n",
        "\n",
        "      encoder_input.append(source_line)\n",
        "      decoder_input.append(target_line_in)\n",
        "      decoder_target.append(target_line_out)\n",
        "\n",
        "      #num_samples로 지정한 데이터의 개수에 도달하면 루프를 종료\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "\n",
        "  return encoder_input,decoder_input, decoder_target"
      ],
      "metadata": {
        "id": "0I-fwjMJDnY6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_w_eng, in_w_fra, out_w_fra = load_preprocessed_data()\n",
        "print('인코더 입력 :', in_w_eng[:5])\n",
        "print('디코더 입력 :', in_w_fra[:5])\n",
        "print('디코더 레이블 :', out_w_fra[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MtBYBloBHSG",
        "outputId": "8287d018-c815-4a72-8694-ee3a01b78c55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
            "디코더 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
            "디코더 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#텍스트 데이터를 정수 시퀀스로 변환하고 패딩 적용\n",
        "\n",
        "#필터=\"\": 특수문자 제거 x / lower=False: 단어를 소문자로 변환 x\n",
        "#텍스트 데이터 내 단어들이 정수로 매핑되는 인덱스 생성\n",
        "#각 문장을 정수 시퀀스로 변환\n",
        "#시퀀스 뒷부분에 패딩 추가\n",
        "tokenizer_eng = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_eng.fit_on_texts(in_w_eng)\n",
        "encoder_input = tokenizer_eng.texts_to_sequences(in_w_eng)\n",
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
        "\n",
        "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_fra.fit_on_texts(in_w_fra)\n",
        "tokenizer_fra.fit_on_texts(out_w_fra)\n",
        "\n",
        "decoder_input = tokenizer_fra.texts_to_sequences(in_w_fra)\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "\n",
        "decoder_target = tokenizer_fra.texts_to_sequences(out_w_fra)\n",
        "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
      ],
      "metadata": {
        "id": "lzt2QsJCB16G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코더 입력 크기 :',encoder_input.shape)\n",
        "print('디코더 입력 크기:',decoder_input.shape)\n",
        "print('디코더 레이블 크기:',decoder_target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh7KXej1dSIH",
        "outputId": "0ac64a7a-8279-4713-bbb9-9816ddd45918"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더 입력 크기 : (100000, 10)\n",
            "디코더 입력 크기: (100000, 17)\n",
            "디코더 레이블 크기: (100000, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = len(tokenizer_eng.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
        "print(\"영어 단어 집합 크기: {:d}, 불어 단어 집합 크기: {:d}\".format(src_vocab_size, tar_vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk8XrarsETU5",
        "outputId": "69f01c0e-96da-450c-e53c-eb90791bd3f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 단어 집합 크기: 8654, 불어 단어 집합 크기: 14583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어->정수 딕셔너리, 정수->단어 딕셔너리 생성\n",
        "src_to_index = tokenizer_eng.word_index\n",
        "index_to_src = tokenizer_eng.index_word\n",
        "tar_to_index = tokenizer_fra.word_index\n",
        "index_to_tar = tokenizer_fra.index_word"
      ],
      "metadata": {
        "id": "NZUzog8jFu3v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델의 학습이 일정한 순서로 진행되지 않고, 전체적인 특성을 학습하도록 랜덤 셔플 사용\n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print(\"랜덤 시퀸스: \", indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27qB3naiGJhP",
        "outputId": "c5347249-f9e0-4b3a-ce1b-43ef9aad9c3a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "랜덤 시퀸스:  [28781  9516 98068 ... 62671 74512 34590]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "metadata": {
        "id": "Pfh5XG_1GYqi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input[40000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7I9olwuGi3B",
        "outputId": "ee91830d-bad8-4d2f-aa68-008e0c04183e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([324,  42, 984,   1,   0,   0,   0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input[40000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N5UyotcHs73",
        "outputId": "a2b6a8ef-af4f-46e2-b673-d3f11b6baa28"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2, 216,  10,  85, 149,   1,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target[40000]\n",
        "\n",
        "#decoder_input과 decoder_target에서 동일 시퀀스 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-q_MjmZHuS5",
        "outputId": "2c4d1aca-716d-475b-fae2-9ee2ab576c87"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([216,  10,  85, 149,   1,   3,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_val = int(100000*0.2)\n",
        "print('검증 데이터 개수: ', n_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7yINZ6GIR7Y",
        "outputId": "e29f08e7-55a7-4444-88bf-f19b74cd1653"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 데이터 개수:  20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터를 훈련데이터와 검증데이터로 분리\n",
        "encoder_input_train = encoder_input[:-n_val]\n",
        "decoder_input_train = decoder_input[:-n_val]\n",
        "decoder_target_train = decoder_target[:-n_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_val:]\n",
        "decoder_input_test = decoder_input[-n_val:]\n",
        "decoder_target_test = decoder_target[-n_val:]"
      ],
      "metadata": {
        "id": "51uZt8oAoTTa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 소스 데이터 크기 :',encoder_input_train.shape)\n",
        "print('훈련 타겟 데이터 크기 :',decoder_input_train.shape)\n",
        "print('훈련 타겟 레이블 크기 :',decoder_target_train.shape)\n",
        "print('검증 소스 데이터 크기 :',encoder_input_test.shape)\n",
        "print('검증 타겟 데이터 크기 :',decoder_input_test.shape)\n",
        "print('검증 타겟 레이블 크기 :',decoder_target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH8zQdybo08m",
        "outputId": "a51320e8-ecf5-4360-c31c-5313ad63914f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 소스 데이터 크기 : (80000, 10)\n",
            "훈련 타겟 데이터 크기 : (80000, 17)\n",
            "훈련 타겟 레이블 크기 : (80000, 17)\n",
            "검증 소스 데이터 크기 : (20000, 10)\n",
            "검증 타겟 데이터 크기 : (20000, 17)\n",
            "검증 타겟 레이블 크기 : (20000, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 설계"
      ],
      "metadata": {
        "id": "boXqkAklpFg5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking"
      ],
      "metadata": {
        "id": "HpoPVx-zpQGr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩 벡터 차원, LSTM의 은닉상태 크기 64\n",
        "embedding_dim = 64\n",
        "hidden_units = 64"
      ],
      "metadata": {
        "id": "Xu5mqObYpZ3p"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#단어를 임베딩 벡터로 변환\n",
        "encoder_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs)\n",
        "\n",
        "#입력 시퀀스에서 값이 0인 부분을 마스킹 -> 패딩 부분 무시\n",
        "encoder_masking = Masking(mask_value=0.0)(encoder_emb)\n",
        "\n",
        "# LSTM 층의 마지막 시점의 은닉 상태와 셀 상태를 반환\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True)\n",
        "\n",
        "#마스킹된 입력 시퀀스를 LSTM 층에 입력으로 전달\n",
        "encoder_outputs, state_h,state_c = encoder_lstm(encoder_masking)\n",
        "\n",
        "#인코더의 은닉상태와 셀상태 저장\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "LZn0zmrApjCp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_emb_layer = Embedding(tar_vocab_size, hidden_units)\n",
        "decoder_emb = decoder_emb_layer(decoder_inputs)\n",
        "decoder_masking = Masking(mask_value=0.0)(decoder_emb)\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "\n",
        "#인코더의 은닉 상태를 초기 은닉상태로 사용\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_masking, initial_state = encoder_states)\n",
        "\n",
        "#출력층을 통해 단어 예측\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "#Seq2Seq 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "xel20OI7qaqc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "FwtWlc5kEEjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=128,\n",
        "          epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7VDLfbor3eK",
        "outputId": "c72185b8-4254-4301-9c67-50504cbb542e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "625/625 [==============================] - 65s 89ms/step - loss: 2.7522 - accuracy: 0.6254 - val_loss: 1.9347 - val_accuracy: 0.7053\n",
            "Epoch 2/50\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 1.7829 - accuracy: 0.7188 - val_loss: 1.6742 - val_accuracy: 0.7297\n",
            "Epoch 3/50\n",
            "625/625 [==============================] - 31s 49ms/step - loss: 1.5808 - accuracy: 0.7429 - val_loss: 1.4983 - val_accuracy: 0.7600\n",
            "Epoch 4/50\n",
            "625/625 [==============================] - 30s 49ms/step - loss: 1.4169 - accuracy: 0.7680 - val_loss: 1.3709 - val_accuracy: 0.7763\n",
            "Epoch 5/50\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 1.2975 - accuracy: 0.7830 - val_loss: 1.2797 - val_accuracy: 0.7872\n",
            "Epoch 6/50\n",
            "625/625 [==============================] - 31s 49ms/step - loss: 1.2041 - accuracy: 0.7942 - val_loss: 1.2062 - val_accuracy: 0.7960\n",
            "Epoch 7/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 1.1268 - accuracy: 0.8025 - val_loss: 1.1472 - val_accuracy: 0.8022\n",
            "Epoch 8/50\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 1.0602 - accuracy: 0.8100 - val_loss: 1.0995 - val_accuracy: 0.8080\n",
            "Epoch 9/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 1.0035 - accuracy: 0.8165 - val_loss: 1.0589 - val_accuracy: 0.8127\n",
            "Epoch 10/50\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.9525 - accuracy: 0.8222 - val_loss: 1.0227 - val_accuracy: 0.8171\n",
            "Epoch 11/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.9048 - accuracy: 0.8275 - val_loss: 0.9890 - val_accuracy: 0.8213\n",
            "Epoch 12/50\n",
            "625/625 [==============================] - 30s 47ms/step - loss: 0.8617 - accuracy: 0.8327 - val_loss: 0.9606 - val_accuracy: 0.8243\n",
            "Epoch 13/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.8224 - accuracy: 0.8375 - val_loss: 0.9358 - val_accuracy: 0.8277\n",
            "Epoch 14/50\n",
            "625/625 [==============================] - 30s 49ms/step - loss: 0.7863 - accuracy: 0.8419 - val_loss: 0.9142 - val_accuracy: 0.8303\n",
            "Epoch 15/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.7530 - accuracy: 0.8464 - val_loss: 0.8930 - val_accuracy: 0.8335\n",
            "Epoch 16/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.7208 - accuracy: 0.8505 - val_loss: 0.8744 - val_accuracy: 0.8360\n",
            "Epoch 17/50\n",
            "625/625 [==============================] - 30s 47ms/step - loss: 0.6909 - accuracy: 0.8546 - val_loss: 0.8575 - val_accuracy: 0.8388\n",
            "Epoch 18/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.6628 - accuracy: 0.8587 - val_loss: 0.8417 - val_accuracy: 0.8417\n",
            "Epoch 19/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.6362 - accuracy: 0.8626 - val_loss: 0.8281 - val_accuracy: 0.8435\n",
            "Epoch 20/50\n",
            "625/625 [==============================] - 30s 47ms/step - loss: 0.6115 - accuracy: 0.8664 - val_loss: 0.8159 - val_accuracy: 0.8457\n",
            "Epoch 21/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.5884 - accuracy: 0.8698 - val_loss: 0.8051 - val_accuracy: 0.8475\n",
            "Epoch 22/50\n",
            "625/625 [==============================] - 30s 47ms/step - loss: 0.5666 - accuracy: 0.8733 - val_loss: 0.7947 - val_accuracy: 0.8493\n",
            "Epoch 23/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.5463 - accuracy: 0.8767 - val_loss: 0.7853 - val_accuracy: 0.8511\n",
            "Epoch 24/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.5270 - accuracy: 0.8802 - val_loss: 0.7795 - val_accuracy: 0.8521\n",
            "Epoch 25/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.5093 - accuracy: 0.8831 - val_loss: 0.7718 - val_accuracy: 0.8536\n",
            "Epoch 26/50\n",
            "625/625 [==============================] - 30s 47ms/step - loss: 0.4922 - accuracy: 0.8861 - val_loss: 0.7651 - val_accuracy: 0.8548\n",
            "Epoch 27/50\n",
            "625/625 [==============================] - 30s 47ms/step - loss: 0.4765 - accuracy: 0.8887 - val_loss: 0.7590 - val_accuracy: 0.8560\n",
            "Epoch 28/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.4619 - accuracy: 0.8916 - val_loss: 0.7546 - val_accuracy: 0.8569\n",
            "Epoch 29/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.4477 - accuracy: 0.8940 - val_loss: 0.7499 - val_accuracy: 0.8580\n",
            "Epoch 30/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.4346 - accuracy: 0.8966 - val_loss: 0.7471 - val_accuracy: 0.8582\n",
            "Epoch 31/50\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.4220 - accuracy: 0.8990 - val_loss: 0.7431 - val_accuracy: 0.8592\n",
            "Epoch 32/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.4101 - accuracy: 0.9014 - val_loss: 0.7404 - val_accuracy: 0.8601\n",
            "Epoch 33/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.3992 - accuracy: 0.9035 - val_loss: 0.7365 - val_accuracy: 0.8608\n",
            "Epoch 34/50\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.3888 - accuracy: 0.9053 - val_loss: 0.7352 - val_accuracy: 0.8610\n",
            "Epoch 35/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.3792 - accuracy: 0.9074 - val_loss: 0.7335 - val_accuracy: 0.8622\n",
            "Epoch 36/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.3700 - accuracy: 0.9092 - val_loss: 0.7305 - val_accuracy: 0.8623\n",
            "Epoch 37/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.3608 - accuracy: 0.9110 - val_loss: 0.7315 - val_accuracy: 0.8626\n",
            "Epoch 38/50\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.3526 - accuracy: 0.9127 - val_loss: 0.7292 - val_accuracy: 0.8634\n",
            "Epoch 39/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.3445 - accuracy: 0.9144 - val_loss: 0.7288 - val_accuracy: 0.8641\n",
            "Epoch 40/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.3370 - accuracy: 0.9158 - val_loss: 0.7278 - val_accuracy: 0.8643\n",
            "Epoch 41/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.3300 - accuracy: 0.9172 - val_loss: 0.7286 - val_accuracy: 0.8644\n",
            "Epoch 42/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.3234 - accuracy: 0.9185 - val_loss: 0.7302 - val_accuracy: 0.8641\n",
            "Epoch 43/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.3168 - accuracy: 0.9199 - val_loss: 0.7284 - val_accuracy: 0.8649\n",
            "Epoch 44/50\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.3106 - accuracy: 0.9211 - val_loss: 0.7293 - val_accuracy: 0.8648\n",
            "Epoch 45/50\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.3049 - accuracy: 0.9223 - val_loss: 0.7290 - val_accuracy: 0.8655\n",
            "Epoch 46/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.2992 - accuracy: 0.9234 - val_loss: 0.7298 - val_accuracy: 0.8654\n",
            "Epoch 47/50\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.2938 - accuracy: 0.9243 - val_loss: 0.7317 - val_accuracy: 0.8654\n",
            "Epoch 48/50\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.2887 - accuracy: 0.9256 - val_loss: 0.7316 - val_accuracy: 0.8656\n",
            "Epoch 49/50\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.2838 - accuracy: 0.9265 - val_loss: 0.7340 - val_accuracy: 0.8660\n",
            "Epoch 50/50\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.2794 - accuracy: 0.9274 - val_loss: 0.7340 - val_accuracy: 0.8661\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ce15afc62c0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터에 대한 정확도 계산\n",
        "train_loss, train_accuracy = model.evaluate(x=[encoder_input_train, decoder_input_train], y=decoder_target_train)\n",
        "print(\"학습 데이터 정확도:\", train_accuracy)\n",
        "\n",
        "# 검증 데이터에 대한 정확도 계산\n",
        "test_loss, test_accuracy = model.evaluate(x=[encoder_input_test, decoder_input_test], y=decoder_target_test)\n",
        "print(\"검증 데이터 정확도:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg1n-ung_HoE",
        "outputId": "53260880-6aff-4149-da39-e0cbd04ac263"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500/2500 [==============================] - 36s 15ms/step - loss: 0.2612 - accuracy: 0.9331\n",
            "학습 데이터 정확도: 0.9331102967262268\n",
            "625/625 [==============================] - 10s 17ms/step - loss: 0.7340 - accuracy: 0.8661\n",
            "검증 데이터 정확도: 0.8661147356033325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "x_len = np.arange(len(y_vloss))\n",
        "plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Testset_loss')\n",
        "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ozQa15vmE55L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seq2seq는 훈련과 테스트 과정에서 동작방식이 다르므로 테스트 과정 위해 모델 다시 설계\n",
        "# -> 번역 단계를 위해 모델 수정하고 동작"
      ],
      "metadata": {
        "id": "xEDHXIgysLJc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(hidden_units,))\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "#임베딩층 재사용\n",
        "decoder_emb2 = decoder_emb_layer(decoder_inputs)\n",
        "\n",
        "#다음 단어 예측위해 이전 시점의 상태를 현 시점의 초기상태로 사용\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "#수정된 디코더\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2\n",
        ")"
      ],
      "metadata": {
        "id": "67xWv3IbtAXD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_sequence(input_seq):\n",
        "  #입력으로부터 인코더의 마지막 시점의 은닉상태, 셀상태 얻기\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  #<SOS>에 해당하는 정수 생성\n",
        "  target_seq = np.zeros((1, 1))\n",
        "  target_seq[0, 0] = tar_to_index['<sos>']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  #<eos>도달 or 정해진 길이를 넘으면 중단\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq]+states_value)\n",
        "\n",
        "    #예측 결과를 단어로\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "    #예측 단어를 문장에 추가\n",
        "    decoded_sentence += ' '+sampled_char\n",
        "    if(sampled_char == '<eos>' or len(decoded_sentence)>50):\n",
        "      stop_condition = True\n",
        "\n",
        "    #현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "    #현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "gvA-nEjrt_BU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#원문 정수 시퀀스->텍스트 시퀀스\n",
        "#인코더 입력으로 사용된 영어 문장을 원래 영어 문장으로 변환\n",
        "def seq_to_src(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0):\n",
        "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "pibI2L6bwGZ0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#번역문 정수 시퀀스->텍스트 시퀀스\n",
        "#디코더의 출력으로 사용된 불어 문장을 원래 불어 문장으로 변환\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    #encoded_word가 0이 아니고, <sos>와 <eos> 토큰이 아니라면 해당 정수를 프랑스어 단어로 변환\n",
        "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
        "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "gudan85wwY2c"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#훈련데이터 대한 임의의 인덱스 샘플 결과 출력\n",
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decoder_sequence(input_seq)\n",
        "\n",
        "  print(\"입력 문장: \", seq_to_src(encoder_input_train[seq_index]))\n",
        "  # 학습 데이터에 있는 불어 문장\n",
        "  print(\"정답 문장: \", seq_to_tar(decoder_input_train[seq_index]))\n",
        "  #디코더 모델에 의해 실제로 생성된 불어 문장\n",
        "  print(\"번역 문장: \", decoded_sentence[1:-5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b88HH0qJxPt-",
        "outputId": "f6bf8e26-5535-4ade-a2c9-d039e63f324a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 371ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "입력 문장:  don t scribble here . \n",
            "정답 문장:  ne gribouille pas ici ! \n",
            "번역 문장:  ne gribouille pas ici ! \n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "입력 문장:  tom do you still love me ? \n",
            "정답 문장:  tom est ce que tu m aimes toujours ? \n",
            "번역 문장:  tom m aimes tu toujours ? \n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "입력 문장:  we stayed at a nice hotel . \n",
            "정답 문장:  nous sejournames dans un chouette hotel . \n",
            "번역 문장:  nous sejournames dans un bel hotel . \n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "입력 문장:  the sound woke me up . \n",
            "정답 문장:  le bruit m a reveille . \n",
            "번역 문장:  le bruit m a reveille . \n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "입력 문장:  my grandfather was a miner . \n",
            "정답 문장:  mon grand pere etait mineur . \n",
            "번역 문장:  mon reste etait grand groupe . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#검증 데이터에 대한 임의의 인덱스 샘플 결과 출력\n",
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decoder_sequence(input_seq)\n",
        "\n",
        "  print(\"입력 문장: \", seq_to_src(encoder_input_test[seq_index]))\n",
        "  print(\"정답 문장: \", seq_to_tar(decoder_input_test[seq_index]))\n",
        "  print(\"번역 문장: \", decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-0_SRMkxwAc",
        "outputId": "c63fc0c7-57ed-480f-9f65-5b2d93194839"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "입력 문장:  we didn t think of that . \n",
            "정답 문장:  nous n y avons pas pense . \n",
            "번역 문장:  nous n en avons pas qu une decision ! \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "입력 문장:  whose phone is this ? \n",
            "정답 문장:  a qui est ce telephone ? \n",
            "번역 문장:  a qui est ce telephone ? \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "입력 문장:  when were you born ? \n",
            "정답 문장:  quand es tu ne ? \n",
            "번역 문장:  quand etes vous nees ? \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "입력 문장:  i didn t take the hint . \n",
            "정답 문장:  je n ai pas compris l allusion . \n",
            "번역 문장:  je ne me suis pas alle au bras . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "입력 문장:  you ve misspelled my name . \n",
            "정답 문장:  vous avez ecorche mon nom . \n",
            "번역 문장:  tu as ecorche mon nom . \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}