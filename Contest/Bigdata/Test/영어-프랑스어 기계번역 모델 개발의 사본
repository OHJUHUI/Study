{"cells":[{"cell_type":"code","source":["# seq2seq를 이용한 기계 번역기 제작"],"metadata":{"id":"e0Pecv2dtG_c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNoR-ydKWh6L"},"outputs":[],"source":["import os\n","import re\n","import shutil\n","import zipfile\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import unicodedata\n","import urllib3\n","from tensorflow.keras.layers import Embedding, GRU, Dense\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uip2poXIYElD"},"outputs":[],"source":["num_samples = 100000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XzFllxhZjHa"},"outputs":[],"source":["# 전처리 과정\n","def to_ascii(s):\n","# 프랑스어 악센트 제거\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","                   if unicodedata.category(c) != 'Mn')\n","\n","def preprocess_sentence(sent):\n","# 단어와 구두점 사이에 공백 추가\n","  sent = to_ascii(sent.lower())\n","\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","# 선언 기호를 제외한 기호는 공백으로 변환\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","# 다수의 공백 -> 하나의 공백\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1689834886936,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"MqQL0_IpZnyM","outputId":"fbf0aa29-52e0-412a-a591-eeaf481397b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["전처리 전 영어 문장 : Have you had dinner?\n","전처리 후 영어 문장 : have you had dinner ?\n","전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n","전처리 후 프랑스어 문장 : avez vous deja dine ?\n"]}],"source":["# 전처리 함수 테스트\n","en_sent = u\"Have you had dinner?\"\n","fr_sent = u\"Avez-vous déjà diné?\"\n","\n","print('전처리 전 영어 문장 :', en_sent)\n","print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n","print('전처리 전 프랑스어 문장 :', fr_sent)\n","print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAe4u72bZ4wW"},"outputs":[],"source":["# 레이블에 해당되는 출력 시퀀스 분리 후 저장\n","# 입력시퀀스 <sos> (-> 시작을 의미하는 토큰)\n","# 출력시퀀스 <eos> (-> 종료를 의미하는 토큰)\n","\n","def load_preprocessed_data():\n","  encoder_input, decoder_input, decoder_target = [], [], []\n","\n","  with open(\"fra.txt\", \"r\") as lines:\n","    for i, line in enumerate(lines):\n","      # source 데이터와 target 데이터 분리\n","      src_line, tar_line, _ = line.strip().split('\\t')\n","\n","      # source 데이터 전처리\n","      src_line = [w for w in preprocess_sentence(src_line).split()]\n","\n","      # target 데이터 전처리\n","      tar_line = preprocess_sentence(tar_line)\n","      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","\n","      if i == num_samples - 1:\n","        break\n","\n","  return encoder_input, decoder_input, decoder_target"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4228,"status":"ok","timestamp":1689834919022,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"QCJZvIQaaI09","outputId":"c232f418-d733-44f4-b2d1-0bfe9f621bf6"},"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n","디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n","디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"]}],"source":["# 인코더의 입력, 디코더의 입력, 디코더의 레이블 상위 5개 샘플 출력\n","sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n","print('인코더의 입력 :',sents_en_in[:5])\n","print('디코더의 입력 :',sents_fra_in[:5])\n","print('디코더의 레이블 :',sents_fra_out[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBHhrdSBaO6J"},"outputs":[],"source":["# 기계 번역 (Machine Translation)을 위한 데이터 전처리를 수행\n","\n","# 영어 데이터 Tokenizer 객체를 생성(lower=False는 영어 문장을 소문자로 변환하지 않음)\n","tokenizer_en = Tokenizer(filters=\"\", lower=False)\n","# sents_en_in을 토큰화하여 단어 사전을 구축\n","tokenizer_en.fit_on_texts(sents_en_in)\n","# 토큰화된 영어 문장들을 정수 시퀀스로 변환\n","encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n","# encoder_input에 대해 패딩을 수행\n","encoder_input = pad_sequences(encoder_input, padding=\"post\")\n","\n","# 프랑스 데이터 새로운 Tokenizer 객체를 생성\n","tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n","# sents_fra_in을 토큰화하여 단어 사전을 구축\n","tokenizer_fra.fit_on_texts(sents_fra_in)\n","# sents_fra_out도 같은 단어 사전을 이용하여 구축\n","tokenizer_fra.fit_on_texts(sents_fra_out)\n","\n","# 토큰화된 프랑스어 입력 문장들을 정수 시퀀스로 변환\n","decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n","# decoder_input에 대해 패딩을 수행\n","decoder_input = pad_sequences(decoder_input, padding=\"post\")\n","\n","# 토큰화된 프랑스어 출력 문장들을 정수 시퀀스로 변환\n","decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n","# decoder_target에 대해 패딩을 수행\n","decoder_target = pad_sequences(decoder_target, padding=\"post\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689834922315,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"jB8-QjQRag0u","outputId":"91332f80-1d7e-4990-a164-cfc94b37fc66"},"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력의 크기(shape) : (100000, 9)\n","디코더의 입력의 크기(shape) : (100000, 17)\n","디코더의 레이블의 크기(shape) : (100000, 17)\n"]}],"source":["# 데이터 크기 확인\n","print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n","print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n","print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1689834923130,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"7DvZy-U-ai0B","outputId":"c159f1df-b9a3-45ba-e67d-11bcbdd01f20"},"outputs":[{"output_type":"stream","name":"stdout","text":["영어 단어 집합의 크기 : 8656, 프랑스어 단어 집합의 크기 : 14584\n"]}],"source":["src_vocab_size = len(tokenizer_en.word_index) + 1\n","tar_vocab_size = len(tokenizer_fra.word_index) + 1\n","print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UH7gr6TanG2"},"outputs":[],"source":["# tokenizer_en 객체를 사용하여 영어 단어를 정수 인덱스로 매핑하는 딕셔너리인 src_to_index를 생성\n","src_to_index = tokenizer_en.word_index\n","# tokenizer_en 객체를 사용하여 정수 인덱스를 영어 단어로 매핑하는 딕셔너리인 index_to_src를 생성\n","index_to_src = tokenizer_en.index_word\n","\n","# tokenizer_fra 객체를 사용하여 프랑스어 단어를 정수 인덱스로 매핑하는 딕셔너리인 tar_to_index를 생성\n","tar_to_index = tokenizer_fra.word_index\n","# tokenizer_fra 객체를 사용하여 정수 인덱스를 프랑스어 단어로 매핑하는 딕셔너리인 index_to_tar를 생성\n","index_to_tar = tokenizer_fra.index_word"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":604,"status":"ok","timestamp":1689834936444,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"-wMqt5zVas6c","outputId":"0619ab5a-369a-4c12-8c97-040289c62fc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["랜덤 시퀀스 :  [14400 13330 19542 ... 62931 17170 69558]\n"]}],"source":["# encoder_input의 행의 개수(데이터 샘플의 수)를 기반으로 0부터 시작하여 연속적인 정수로 채워진 넘파이 배열 indices를 생성\n","indices = np.arange(encoder_input.shape[0])\n","# np.random.shuffle 함수를 사용하여 indices 배열의 원소를 임의로 섞기\n","np.random.shuffle(indices)\n","print('랜덤 시퀀스 : ', indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZGhVrUtrbX3K"},"outputs":[],"source":["# encoder_input의 데이터가 랜덤한 순서로 재배열\n","encoder_input = encoder_input[indices]\n","# decoder_input 배열도 indices 배열의 순서에 맞추어 재정렬\n","decoder_input = decoder_input[indices]\n","# decoder_target 배열도 indices 배열의 순서에 맞추어 재정렬\n","decoder_target = decoder_target[indices]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1689834938426,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"K17xT5RLcAtz","outputId":"b47a39d4-f76d-4490-9007-e9dcb7bfb2e9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   3,   16,   78, 1818,    1,    0,    0,    0,    0], dtype=int32)"]},"metadata":{},"execution_count":14}],"source":["encoder_input[5000]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689834938426,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"Lb9mnaKlcCG3","outputId":"7f0b7c2d-053c-481f-bd5f-e3de0ef42cc5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  2,  14,  41,  89, 790,   1,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0], dtype=int32)"]},"metadata":{},"execution_count":15}],"source":["decoder_input[5000]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689834938427,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"A7_zLemccEc6","outputId":"8f38717f-9125-486a-91a6-b13bd73c6777"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 14,  41,  89, 790,   1,   3,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0], dtype=int32)"]},"metadata":{},"execution_count":16}],"source":["decoder_target[5000]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":564,"status":"ok","timestamp":1689834945083,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"GZL1ei6lcGKl","outputId":"2fbea978-a997-42eb-dd69-820b56885f70"},"outputs":[{"output_type":"stream","name":"stdout","text":["검증 데이터의 개수 : 10000\n"]}],"source":["# 훈련 데이터의 10%를 테스트 데이터로 분리\n","n_of_val = int(100000*0.1)\n","print('검증 데이터의 개수 :',n_of_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvmDinzocH63"},"outputs":[],"source":[" # 검증 데이터의 개수 10,000개의 데이터를 테스트 데이터로 사용\n","encoder_input_train = encoder_input[:-n_of_val]\n","decoder_input_train = decoder_input[:-n_of_val]\n","decoder_target_train = decoder_target[:-n_of_val]\n","\n","encoder_input_test = encoder_input[-n_of_val:]\n","decoder_input_test = decoder_input[-n_of_val:]\n","decoder_target_test = decoder_target[-n_of_val:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1689834955749,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"},"user_tz":-540},"id":"guepvtFpcJuS","outputId":"20223b7d-b61b-460e-bf9b-148041b54388"},"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 source 데이터의 크기 : (90000, 9)\n","훈련 target 데이터의 크기 : (90000, 17)\n","훈련 target 레이블의 크기 : (90000, 17)\n","테스트 source 데이터의 크기 : (10000, 9)\n","테스트 target 데이터의 크기 : (10000, 17)\n","테스트 target 레이블의 크기 : (10000, 17)\n"]}],"source":["# 훈련데이터와 테스트 데이터의 크기 출력\n","print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n","print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n","print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n","print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n","print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n","print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpsNrfe_cMAF"},"outputs":[],"source":["# 기계 번역기 만들기\n","# 임베딩 벡터의 차원과 LSTM의 은닉 상태의 크기를 64로 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQVBY1qJ9aKf"},"outputs":[],"source":["from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n","from tensorflow.keras.models import Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUjrLjo49ayT"},"outputs":[],"source":["embedding_dim = 64\n","hidden_units = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aG_IEPk-a-0"},"outputs":[],"source":["encoder_inputs = Input(shape=(None,))\n","# Embedding 레이어를 생성하여 입력 시퀀스를 임베딩 벡터로 변환\n","enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs)\n","# Masking 레이어를 생성하여 입력 시퀀스에서 패딩을 마스킹\n","enc_masking = Masking(mask_value=0.0)(enc_emb)\n","encoder_lstm = LSTM(hidden_units, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n","encoder_states = [state_h, state_c]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gkSfDsG-chp"},"outputs":[],"source":["decoder_inputs = Input(shape=(None,))\n","dec_emb_layer = Embedding(tar_vocab_size, hidden_units)\n","dec_emb = dec_emb_layer(decoder_inputs)\n","dec_masking = Masking(mask_value=0.0)(dec_emb)\n","\n","decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n","\n","decoder_outputs, _, _ = decoder_lstm(dec_masking,\n","                                     initial_state=encoder_states)\n","\n","decoder_dense = Dense(tar_vocab_size, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H5hL7xI_-f0X","outputId":"1eec4c99-5548-4bc7-fd28-dde0a98acce3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","704/704 [==============================] - 68s 81ms/step - loss: 2.6348 - acc: 0.6426 - val_loss: 1.8740 - val_acc: 0.7111\n","Epoch 2/50\n","704/704 [==============================] - 35s 50ms/step - loss: 1.7258 - acc: 0.7236 - val_loss: 1.6107 - val_acc: 0.7419\n","Epoch 3/50\n","704/704 [==============================] - 33s 47ms/step - loss: 1.4943 - acc: 0.7582 - val_loss: 1.4142 - val_acc: 0.7729\n","Epoch 4/50\n","704/704 [==============================] - 35s 50ms/step - loss: 1.3176 - acc: 0.7823 - val_loss: 1.2738 - val_acc: 0.7904\n","Epoch 5/50\n","704/704 [==============================] - 32s 46ms/step - loss: 1.1884 - acc: 0.7978 - val_loss: 1.1748 - val_acc: 0.8026\n","Epoch 6/50\n","704/704 [==============================] - 33s 47ms/step - loss: 1.0872 - acc: 0.8093 - val_loss: 1.0986 - val_acc: 0.8105\n","Epoch 7/50\n","704/704 [==============================] - 33s 47ms/step - loss: 1.0061 - acc: 0.8182 - val_loss: 1.0370 - val_acc: 0.8173\n","Epoch 8/50\n","704/704 [==============================] - 33s 48ms/step - loss: 0.9379 - acc: 0.8261 - val_loss: 0.9903 - val_acc: 0.8226\n","Epoch 9/50\n","704/704 [==============================] - 32s 46ms/step - loss: 0.8794 - acc: 0.8329 - val_loss: 0.9473 - val_acc: 0.8275\n","Epoch 10/50\n","704/704 [==============================] - 34s 49ms/step - loss: 0.8268 - acc: 0.8394 - val_loss: 0.9120 - val_acc: 0.8319\n","Epoch 11/50\n","704/704 [==============================] - 33s 47ms/step - loss: 0.7791 - acc: 0.8452 - val_loss: 0.8781 - val_acc: 0.8368\n","Epoch 12/50\n","704/704 [==============================] - 33s 47ms/step - loss: 0.7353 - acc: 0.8509 - val_loss: 0.8510 - val_acc: 0.8398\n","Epoch 13/50\n","704/704 [==============================] - 32s 46ms/step - loss: 0.6955 - acc: 0.8562 - val_loss: 0.8249 - val_acc: 0.8435\n","Epoch 14/50\n","704/704 [==============================] - 33s 46ms/step - loss: 0.6587 - acc: 0.8613 - val_loss: 0.8038 - val_acc: 0.8471\n","Epoch 15/50\n","704/704 [==============================] - 32s 45ms/step - loss: 0.6252 - acc: 0.8663 - val_loss: 0.7832 - val_acc: 0.8498\n","Epoch 16/50\n","704/704 [==============================] - 32s 46ms/step - loss: 0.5949 - acc: 0.8707 - val_loss: 0.7660 - val_acc: 0.8531\n","Epoch 17/50\n","704/704 [==============================] - 33s 47ms/step - loss: 0.5668 - acc: 0.8750 - val_loss: 0.7511 - val_acc: 0.8553\n","Epoch 18/50\n","  9/704 [..............................] - ETA: 26s - loss: 0.5248 - acc: 0.8819"]}],"source":["model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n","          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n","          batch_size=128, epochs=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-JcfeFH-mJe"},"outputs":[],"source":["# Seq2Seq (Sequence-to-Sequence) 기계 번역 모델의 인코더와 디코더 부분을 정의하고, 모델을 생성\n","\n","# 인토더 모델 정의\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(hidden_units,))\n","decoder_state_input_c = Input(shape=(hidden_units,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n","decoder_states2 = [state_h2, state_c2]\n","\n","decoder_outputs2 = decoder_dense(decoder_outputs2)\n","\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs2] + decoder_states2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d4s4zq2xAHkd"},"outputs":[],"source":["def decode_sequence(inpur_seq):\n","  states_value = encoder_model.predict(input_seq)\n","\n","  target_seq = np.zeros((1, 1))\n","  target_seq[0, 0] = tar_to_index['<sos>']\n","\n","  stop_condition = False\n","  decoded_sentence = ''\n","\n","  while not stop_condition:\n","    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_char = index_to_tar[sampled_token_index]\n","\n","    decoded_sentence += \" \" + sampled_char\n","\n","    if (sampled_char == '<eos>' or len(decoded_sentence) > 50 ) :\n","      stop_condition = True\n","\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = sampled_token_index\n","\n","    states_value = [h, c]\n","\n","  return decoded_sentence\n"]},{"cell_type":"code","source":["# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_src(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0):\n","      sentence = sentence + index_to_src[encoded_word] + ' '\n","  return sentence\n","\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_tar(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n","      sentence = sentence + index_to_tar[encoded_word] + ' '\n","  return sentence"],"metadata":{"id":"UR17Drxebna5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 훈련 데이터에 대해 임의 선택한 인덱스의 샘플 출력\n","for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_train[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print(\"-\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5x8QYoAcgLE","executionInfo":{"status":"ok","timestamp":1689833051200,"user_tz":-540,"elapsed":4058,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}},"outputId":"29bddb35-dc0f-4399-bde3-98417af63c85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 1s/step\n","1/1 [==============================] - 0s 344ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","입력문장 : are you an only child ? \n","정답문장 : es tu un enfant unique ? \n","번역문장 : es tu un enfant unique ? \n","--------------------------------------------------\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","입력문장 : this dog will protect us . \n","정답문장 : ce chien va nous proteger . \n","번역문장 : ce chien nous manquera . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 19ms/step\n","입력문장 : i kept on reading . \n","정답문장 : j ai continue a lire . \n","번역문장 : j ai continue a lire . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","입력문장 : stop playing hard to get . \n","정답문장 : cessez de faire celles qui ne sont pas interessees ! \n","번역문장 : arrete de faire ca signe . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","입력문장 : i brought you flowers . \n","정답문장 : je t ai apporte des fleurs . \n","번역문장 : je t ai apporte des fleurs . \n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":["# 테스트 데이터에서 임의 선택한 인덱스의 샘플 결과 출력\n","for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_test[seq_index: seq_index + 1]\n","  decoded_sentence = decode_sequence(input_seq)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n","  print(\"번역문장 :\",decoded_sentence[1:-5])\n","  print(\"-\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAh29eN2qz6H","executionInfo":{"status":"ok","timestamp":1689833054358,"user_tz":-540,"elapsed":3173,"user":{"displayName":"(호서대학교)오주희","userId":"13411961940056535669"}},"outputId":"309d7778-7dc4-4999-fed4-55877b39e7c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 33ms/step\n","입력문장 : tom looks devastated . \n","정답문장 : tom a l air aneanti . \n","번역문장 : tom semble siffler . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 31ms/step\n","입력문장 : i made some calls . \n","정답문장 : j ai passe quelques coups de fil . \n","번역문장 : j ai pris quelque chose . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","입력문장 : you re careful . \n","정답문장 : tu es prudent . \n","번역문장 : vous etes prudent . \n","--------------------------------------------------\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 34ms/step\n","입력문장 : just ignore it . \n","정답문장 : vous n avez qu a l ignorer ! \n","번역문장 : ignore le simplement ! \n","--------------------------------------------------\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 34ms/step\n","입력문장 : i feel betrayed . \n","정답문장 : je me sens trahie . \n","번역문장 : je me sens trahi . \n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":["from nltk.translate.bleu_score import sentence_bleu"],"metadata":{"id":"xkdz4izrDZoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델을 사용하여 테스트 데이터셋에 대한 예측 수행\n","predicted_sequences = model.predict(encoder_input_test)\n","\n","# 정확도 측정을 위한 함수\n","def calculate_accuracy(targets, predictions):\n","    correct_predictions = 0\n","    total_predictions = len(targets)\n","    for i in range(total_predictions):\n","        if np.array_equal(targets[i], predictions[i]):\n","            correct_predictions += 1\n","    return correct_predictions / total_predictions\n","\n","# 정확도 측정\n","accuracy = calculate_accuracy(decoder_target_test, predicted_sequences)\n","print(f\"테스트 데이터셋 정확도: {accuracy * 100:.2f}%\")"],"metadata":{"id":"tuJm9TkVtFRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JBRl0U3d-qMO"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1tQsBuum0EUAWND-0nWEzUOD5huVznoSB","timestamp":1689835561188}],"gpuType":"T4","authorship_tag":"ABX9TyMxKJg3jhsKqnBCjATIHMHa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}