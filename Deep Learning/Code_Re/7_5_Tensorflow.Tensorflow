{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNyt1rStIuXaeuukRH6oj1f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0sGx_4jq9-bf"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models, optimizers, initializers\n","import numpy as np\n","\n","# 하이퍼파라미터 설정\n","D_i, D_k, D_o = 10, 40, 5\n","\n","# 모델 정의\n","inputs = tf.keras.Input(shape=(D_i,))\n","h1 = layers.Dense(D_k, activation='relu', kernel_initializer=initializers.HeNormal())(inputs)\n","h2 = layers.Dense(D_k, activation='relu', kernel_initializer=initializers.HeNormal())(h1)\n","outputs = layers.Dense(D_o, kernel_initializer=initializers.HeNormal())(h2)\n","model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n","\n","# 가중치 초기화 함수\n","def weights_init(layer_in):\n","    if isinstance(layer_in, layers.Dense):\n","        layer_in.kernel.assign(initializers.HeNormal()(layer_in.kernel.shape))\n","        layer_in.bias.assign(initializers.Zeros()(layer_in.bias.shape))\n","\n","model.layers[-1].kernel_initializer = initializers.HeNormal()\n","model.layers[-1].bias_initializer = initializers.Zeros()\n","\n","# 손실 함수 및 옵티마이저 설정\n","criterion = tf.keras.losses.MeanSquaredError()\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n","\n","# 학습률 스케줄러 설정\n","scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=0.1,\n","    decay_steps=10,\n","    decay_rate=0.5,\n","    staircase=True\n",")\n","\n","# 데이터 준비\n","x = np.random.randn(100, D_i)\n","y = np.random.randn(100, D_o)\n","dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(10).shuffle(buffer_size=100, seed=42)\n","\n","# 학습 루프\n","for epoch in range(100):\n","    epoch_loss = 0.0\n","\n","    for step, (x_batch, y_batch) in enumerate(dataset):\n","        with tf.GradientTape() as tape:\n","            pred = model(x_batch, training=True)\n","            loss = criterion(y_batch, pred)\n","        grads = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","        epoch_loss += loss.numpy()\n","\n","    print(f'Epoch {epoch:5d}, loss {epoch_loss:.3f}')\n","\n","    # 학습률 스케줄러 업데이트\n","    optimizer.learning_rate = scheduler(epoch)\n"]}]}