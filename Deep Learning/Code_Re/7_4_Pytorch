{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9un/yEJCz5joNoARD1pjd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8YlvxVmhAs_F","executionInfo":{"status":"ok","timestamp":1717045452438,"user_tz":-540,"elapsed":9518,"user":{"displayName":"(빅데이터경영공학부)박현수","userId":"00571392012820276449"}},"outputId":"0e15a8b1-3f5e-4db6-cf52-933f9a512e9f"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-5b99d4b5dd9e>:19: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n","  nn.init.kaiming_uniform(layer_in.weight)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch     0, loss 15.686\n","Epoch     1, loss 10.797\n","Epoch     2, loss 9.255\n","Epoch     3, loss 8.655\n","Epoch     4, loss 7.669\n","Epoch     5, loss 6.808\n","Epoch     6, loss 6.367\n","Epoch     7, loss 5.786\n","Epoch     8, loss 5.331\n","Epoch     9, loss 5.560\n","Epoch    10, loss 5.170\n","Epoch    11, loss 4.489\n","Epoch    12, loss 4.060\n","Epoch    13, loss 3.746\n","Epoch    14, loss 3.523\n","Epoch    15, loss 3.264\n","Epoch    16, loss 3.149\n","Epoch    17, loss 3.055\n","Epoch    18, loss 3.022\n","Epoch    19, loss 3.010\n","Epoch    20, loss 2.668\n","Epoch    21, loss 2.592\n","Epoch    22, loss 2.524\n","Epoch    23, loss 2.471\n","Epoch    24, loss 2.342\n","Epoch    25, loss 2.279\n","Epoch    26, loss 2.238\n","Epoch    27, loss 2.191\n","Epoch    28, loss 2.175\n","Epoch    29, loss 2.138\n","Epoch    30, loss 2.098\n","Epoch    31, loss 2.069\n","Epoch    32, loss 2.056\n","Epoch    33, loss 2.036\n","Epoch    34, loss 2.024\n","Epoch    35, loss 2.008\n","Epoch    36, loss 1.997\n","Epoch    37, loss 1.983\n","Epoch    38, loss 1.970\n","Epoch    39, loss 1.961\n","Epoch    40, loss 1.940\n","Epoch    41, loss 1.936\n","Epoch    42, loss 1.933\n","Epoch    43, loss 1.922\n","Epoch    44, loss 1.919\n","Epoch    45, loss 1.912\n","Epoch    46, loss 1.907\n","Epoch    47, loss 1.904\n","Epoch    48, loss 1.900\n","Epoch    49, loss 1.893\n","Epoch    50, loss 1.884\n","Epoch    51, loss 1.881\n","Epoch    52, loss 1.878\n","Epoch    53, loss 1.876\n","Epoch    54, loss 1.873\n","Epoch    55, loss 1.870\n","Epoch    56, loss 1.868\n","Epoch    57, loss 1.867\n","Epoch    58, loss 1.863\n","Epoch    59, loss 1.862\n","Epoch    60, loss 1.857\n","Epoch    61, loss 1.855\n","Epoch    62, loss 1.855\n","Epoch    63, loss 1.853\n","Epoch    64, loss 1.853\n","Epoch    65, loss 1.851\n","Epoch    66, loss 1.851\n","Epoch    67, loss 1.849\n","Epoch    68, loss 1.848\n","Epoch    69, loss 1.846\n","Epoch    70, loss 1.844\n","Epoch    71, loss 1.844\n","Epoch    72, loss 1.843\n","Epoch    73, loss 1.842\n","Epoch    74, loss 1.841\n","Epoch    75, loss 1.840\n","Epoch    76, loss 1.840\n","Epoch    77, loss 1.839\n","Epoch    78, loss 1.838\n","Epoch    79, loss 1.838\n","Epoch    80, loss 1.837\n","Epoch    81, loss 1.836\n","Epoch    82, loss 1.836\n","Epoch    83, loss 1.836\n","Epoch    84, loss 1.835\n","Epoch    85, loss 1.835\n","Epoch    86, loss 1.834\n","Epoch    87, loss 1.834\n","Epoch    88, loss 1.834\n","Epoch    89, loss 1.834\n","Epoch    90, loss 1.833\n","Epoch    91, loss 1.833\n","Epoch    92, loss 1.833\n","Epoch    93, loss 1.833\n","Epoch    94, loss 1.832\n","Epoch    95, loss 1.832\n","Epoch    96, loss 1.832\n","Epoch    97, loss 1.832\n","Epoch    98, loss 1.832\n","Epoch    99, loss 1.831\n"]}],"source":["import torch\n","import torch, torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.optim.lr_scheduler import StepLR\n","\n","\n","D_i, D_k, D_o = 10, 40, 5\n","\n","model = nn.Sequential(\n","    nn.Linear(D_i, D_k),\n","    nn.ReLU(),\n","    nn.Linear(D_k, D_k),\n","    nn.ReLU(),\n","    nn.Linear(D_k, D_o))\n","\n","\n","def weights_init(layer_in):\n","    if isinstance(layer_in, nn.Linear):\n","        nn.init.kaiming_uniform(layer_in.weight)\n","        layer_in.bias.data.fill_(0.0)\n","model.apply(weights_init)\n","\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n","scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n","\n","# create 100 random data points and store in data loader class\n","x = torch.randn(100, D_i)\n","y = torch.randn(100, D_o)\n","data_loader = DataLoader(TensorDataset(x,y), batch_size=10, shuffle=True)\n","\n","\n","for epoch in range(100):\n","    epoch_loss = 0.0\n","    for i, data in enumerate(data_loader):\n","        x_batch, y_batch = data\n","        optimizer.zero_grad()\n","        pred = model(x_batch)\n","        loss = criterion(pred, y_batch)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","    print(f'Epoch {epoch:5d}, loss {epoch_loss:.3f}')\n","    scheduler.step()\n"]}]}