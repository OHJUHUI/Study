{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap07/7_3_Initialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"L6chybAVFJW2"},"source":["# **Notebook 7.3: Initialization**\n","\n","This notebook explores weight initialization in deep neural networks as described in section 7.5 of the book.\n","\n","Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n","\n","Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LdIDglk1FFcG"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["\\"],"metadata":{"id":"AZRTfvUox0cb"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"Q_CPh9kHx0eW"}},{"cell_type":"markdown","metadata":{"id":"nnUoI0m6GyjC"},"source":["### 1. 신경망 계산을 위한 함수 지정"]},{"cell_type":"markdown","source":["- 파라미터 초기화 함수"],"metadata":{"id":"JCFqTJt42Yb_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVM4Tc_jGI0Q"},"outputs":[],"source":["def init_params(K, D, sigma_sq_omega):\n","  # 항상 같은 난수를 얻도록 시드 설정\n","  np.random.seed(0)\n","\n","  # 입력층\n","  D_i = 1\n","  # 출력층\n","  D_o = 1\n","\n","  # 가중치와 바이어스의 리스트 생성\n","  all_weights = [None] * (K+1)\n","  all_biases = [None] * (K+1)\n","\n","  # 입력층과 출력층의 가중치 및 바이어스 초기화\n","  # np.random.normal: 평균이 0이고 표준편차가 1인 정규분포\n","  # 표준편차는 np.sqrt(sigma_sq_omega)\n","  all_weights[0] = np.random.normal(size=(D, D_i)) * np.sqrt(sigma_sq_omega)\n","  all_weights[-1] = np.random.normal(size=(D_o, D)) * np.sqrt(sigma_sq_omega)\n","  all_biases[0] = np.zeros((D,1))\n","  all_biases[-1]= np.zeros((D_o,1))\n","\n","  # 은닉층의 가중치 및 바이어스 초기화\n","  for layer in range(1,K):\n","    all_weights[layer] = np.random.normal(size=(D,D)) * np.sqrt(sigma_sq_omega)\n","    all_biases[layer] = np.zeros((D,1))\n","\n","  return all_weights, all_biases"]},{"cell_type":"markdown","source":["- neural network 함수"],"metadata":{"id":"hC74A2ut2cVN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZh-7bPXIDq4"},"outputs":[],"source":["# Define ReLU Function\n","def ReLU(preactivation):\n","  activation = preactivation.clip(0.0)\n","  return activation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgquJUJvJPaN"},"outputs":[],"source":["def compute_network_output(net_input, all_weights, all_biases):\n","\n","  K = len(all_weights) -1\n","\n","  all_f = [None] * (K + 1)\n","  all_h = [None] * (K + 1)\n","\n","  all_h[0] = net_input\n","\n","  for layer in range(K): # 0~k-1\n","\n","      all_f[layer] = all_biases[layer] + np.matmul(all_weights[layer], all_h[layer])\n","\n","      all_h[layer+1] = ReLU(all_f[layer])\n","\n","  all_f[K] = all_biases[K] + np.matmul(all_weights[K], all_h[K])\n","\n","  net_output = all_f[K]\n","\n","  return net_output, all_f, all_h"]},{"cell_type":"markdown","source":["\\"],"metadata":{"id":"UnLI4rcuyHbQ"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"PeezMq9CyHfC"}},{"cell_type":"markdown","metadata":{"id":"bIUrcXnOqChl"},"source":["### 2. 신경망 계산 - Initialization for forward pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A55z3rKBqO7M","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eafb7163-6b4e-4832-965c-9b294c896b3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Layer 0, std of hidden units = 0.981\n","Layer 1, std of hidden units = 0.811\n","Layer 2, std of hidden units = 1.472\n","Layer 3, std of hidden units = 4.547\n","Layer 4, std of hidden units = 8.896\n"]}],"source":["# 신경망 구조\n","K = 5\n","D = 8\n","D_i = 1\n","D_o = 1\n","\n","# 초기 분산\n","sigma_sq_omega = 1.0\n","\n","# Initialize parameters\n","all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n","\n","# 입력값 지정\n","n_data = 1000\n","data_in = np.random.normal(size=(1,n_data))\n","\n","# 신경망 계산\n","net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n","\n","# 각 은닉층의 표준편차 비교\n","for layer in range(K):\n","  print(\"Layer %d, std of hidden units = %3.3f\"%(layer, np.std(all_h[layer]))) # 활성화 함수의 표준편차 출력"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"IdV_zChRy1Rd"}},{"cell_type":"markdown","source":["2-1. 초기화 문제 - forward pass"],"metadata":{"id":"9aBCiZCGy1Zy"}},{"cell_type":"markdown","source":["**TO DO**\n","\n","\n","> 1. 층 당 80개의 은닉유닛이 있는 50개의 층으로 변경해라\n","2. sigma_sq_omega를 조절하여 forward pass 계산의 분산이 폭발하는 것을 막아라\n","\n"],"metadata":{"id":"MrQXjLklalBl"}},{"cell_type":"markdown","source":["- He 초기화를 활용한 forward pass"],"metadata":{"id":"M1WLT36I1svu"}},{"cell_type":"code","source":["def he_init_params(K, D):\n","  np.random.seed(0)\n","\n","  D_i = 1\n","  D_o = 1\n","\n","  all_weights = [None] * (K+1)\n","  all_biases = [None] * (K+1)\n","\n","  all_weights[0] = np.random.normal(size=(D, D_i)) * np.sqrt(2 / D_i)\n","  all_weights[-1] = np.random.normal(size=(D_o, D)) * np.sqrt(2 / D)\n","  all_biases[0] = np.zeros((D,1))\n","  all_biases[-1]= np.zeros((D_o,1))\n","\n","  for layer in range(1,K):\n","    all_weights[layer] = np.random.normal(size=(D,D)) * np.sqrt(2 / D)\n","    all_biases[layer] = np.zeros((D,1))\n","\n","  return all_weights, all_biases\n","\n","\n","K= 50\n","D= 80\n","all_weights, all_biases = he_init_params(K,D)\n","net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n","\n","for layer in range(K):\n","  print(\"Layer %d, std of hidden units = %3.3f\"%(layer, np.std(all_h[layer])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IRfmGs8Q5QSn","outputId":"1dd4a7bf-be1d-4f72-d84f-0fe63634b896"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer 0, std of hidden units = 0.981\n","Layer 1, std of hidden units = 0.886\n","Layer 2, std of hidden units = 0.698\n","Layer 3, std of hidden units = 0.741\n","Layer 4, std of hidden units = 0.899\n","Layer 5, std of hidden units = 0.995\n","Layer 6, std of hidden units = 0.886\n","Layer 7, std of hidden units = 0.826\n","Layer 8, std of hidden units = 0.851\n","Layer 9, std of hidden units = 0.737\n","Layer 10, std of hidden units = 0.646\n","Layer 11, std of hidden units = 0.677\n","Layer 12, std of hidden units = 0.597\n","Layer 13, std of hidden units = 0.582\n","Layer 14, std of hidden units = 0.583\n","Layer 15, std of hidden units = 0.572\n","Layer 16, std of hidden units = 0.588\n","Layer 17, std of hidden units = 0.625\n","Layer 18, std of hidden units = 0.672\n","Layer 19, std of hidden units = 0.656\n","Layer 20, std of hidden units = 0.851\n","Layer 21, std of hidden units = 0.719\n","Layer 22, std of hidden units = 0.720\n","Layer 23, std of hidden units = 0.758\n","Layer 24, std of hidden units = 0.868\n","Layer 25, std of hidden units = 0.951\n","Layer 26, std of hidden units = 0.903\n","Layer 27, std of hidden units = 0.772\n","Layer 28, std of hidden units = 0.748\n","Layer 29, std of hidden units = 0.930\n","Layer 30, std of hidden units = 1.064\n","Layer 31, std of hidden units = 0.888\n","Layer 32, std of hidden units = 0.628\n","Layer 33, std of hidden units = 0.596\n","Layer 34, std of hidden units = 0.593\n","Layer 35, std of hidden units = 0.615\n","Layer 36, std of hidden units = 0.549\n","Layer 37, std of hidden units = 0.603\n","Layer 38, std of hidden units = 0.517\n","Layer 39, std of hidden units = 0.517\n","Layer 40, std of hidden units = 0.525\n","Layer 41, std of hidden units = 0.565\n","Layer 42, std of hidden units = 0.618\n","Layer 43, std of hidden units = 0.572\n","Layer 44, std of hidden units = 0.607\n","Layer 45, std of hidden units = 0.719\n","Layer 46, std of hidden units = 0.684\n","Layer 47, std of hidden units = 0.726\n","Layer 48, std of hidden units = 0.612\n","Layer 49, std of hidden units = 0.566\n"]}]},{"cell_type":"markdown","source":["\\"],"metadata":{"id":"eMi2n52fzYH0"}},{"cell_type":"markdown","metadata":{"id":"SxVTKp3IcoBF"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","source":["### 3. Initialization for backward pass"],"metadata":{"id":"kFEUhTeLzYJ7"}},{"cell_type":"markdown","source":["- 손실 함수 정의 및 출력에 대한 손실함수의 도함수 계산"],"metadata":{"id":"LOgEHRmrzpyZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XqWSYWJdhQR"},"outputs":[],"source":["def least_squares_loss(net_output, y):\n","  return np.sum((net_output-y) * (net_output-y))\n","\n","def d_loss_d_output(net_output, y):\n","  return 2*(net_output -y);"]},{"cell_type":"markdown","source":["- backward pass 함수 정의"],"metadata":{"id":"bPOw21SDL18K"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJng7WpRPLMz"},"outputs":[],"source":["# We'll need the indicator function\n","def indicator_function(x):\n","  x_in = np.array(x)\n","  x_in[x_in>=0] = 1\n","  x_in[x_in<0] = 0\n","  return x_in\n","\n","# backward pass\n","def backward_pass(all_weights, all_biases, all_f, all_h, y):  # y는 신경망의 실제 출력값\n","  # 각 층의 가중치와 바이어스 대한 손실 함수의 미분값 리스트\n","  all_dl_dweights = [None] * (K+1)\n","  all_dl_dbiases = [None] * (K+1)\n","  # 각 층의 전 활성화값과 활성화값 대한 손실 함수의 미분값 리스트\n","  all_dl_df = [None] * (K+1)\n","  all_dl_dh = [None] * (K+1)\n","\n","  # all_h[0]은 입력이고 all_f[k]는 출력\n","\n","  # 출력에 대한 손실 함수의 미분 계산(all_f의 k층에 저장)\n","  all_dl_df[K] = np.array(d_loss_d_output(all_f[K],y))\n","\n","  # 네트워크를 통해 역방향으로 계산\n","  for layer in range(K,-1,-1): # 출력층부터 입력층까지 각 층을 거꾸로 반복\n","    # 현재 층(layer)의 바이어스에 대한 손실 함수의 미분을 계산\n","    all_dl_dbiases[layer] = np.array(all_dl_df[layer])\n","    # 현재 층(layer)의 가중치에 대한 손실 함수의 미분을 계산\n","    all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].T)\n","\n","    # 현재 층(layer)의 활성화값에 대한 손실 함수의 미분을 계산\n","    all_dl_dh[layer] = np.matmul(all_weights[layer].T, all_dl_df[layer])\n","\n","    # Calculate the derivatives of the pre-activation f with respect to activation h (eq 7.13, line 3, first part)\n","    if layer > 0:\n","      all_dl_df[layer-1] = indicator_function(all_f[layer-1]) * all_dl_dh[layer]\n","\n","  return all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df"]},{"cell_type":"markdown","metadata":{"id":"phFnbthqwhFi"},"source":["- 신경망 구조에 따른 backward pass를 통한 각 층의 미분 값"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9A9MHc4sQvbp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b197d479-76a0-42f6-ecd6-19d71b96ee2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Layer 1, std of dl_dh = 446.654\n","Layer 2, std of dl_dh = 340.657\n","Layer 3, std of dl_dh = 109.132\n","Layer 4, std of dl_dh = 56.472\n"]}],"source":["# 신경망 구조\n","K = 5\n","D = 8\n","D_i = 1\n","D_o = 1\n","sigma_sq_omega = 1.0\n","\n","# 파라미터 초기화\n","all_weights, all_biases = init_params(K,D,sigma_sq_omega)\n","\n","# 각 층의 사전 활성화 함수에 대한 손실함수의 미분값 저장\n","n_data = 100 # 데이터 포인트 수\n","aggregate_dl_df = [None] * (K+1)\n","\n","for layer in range(1,K): # 1~K(4) ->  은닉층\n","  aggregate_dl_df[layer] = np.zeros((D,n_data))\n","  # 층별로 (8, 100) 만큼 공간 확보 -> 은닉뉴런수가 8개고 데이터 포인터가 100개 이기 때문\n","\n","# 데이터 포인트 수 만큼 반복\n","for c_data in range(n_data):\n","\n","  data_in = np.random.normal(size=(1,1))\n","  y = np.zeros((1,1))\n","\n","  net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n","  all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df = backward_pass(all_weights, all_biases, all_f, all_h, y)\n","\n","  for layer in range(1,K): # 1~K(4) ->  은닉층\n","    aggregate_dl_df[layer][:,c_data] = np.squeeze(all_dl_df[layer])\n","\n","# 각 층의 미분값에 대한 손실함수의 도함수 표준편차 비교\n","for layer in range(1,K):\n","  print(\"Layer %d, std of dl_dh = %3.3f\"%(layer, np.std(aggregate_dl_df[layer].ravel())))\n","  # ravel(): 다차원의 배열을 1차원의 배열로\n","  # -> 배열의 차원을 무시하고 모든 요소의 표준 편차을 구하기 위해"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"al-9cn4k1OYo"}},{"cell_type":"markdown","source":["3-1. 초기화 문제 - Backward pass"],"metadata":{"id":"HgxJMu_R1ObE"}},{"cell_type":"markdown","source":["**TO DO**\n","\n","\n","> 1. 층 당 80개의 은닉유닛이 있는 50개의 층으로 변경해라\n","2. sigma_sq_omega를 조절하여 도함수의 분산이 폭발하는 것을 막아라"],"metadata":{"id":"Qx_cn5ekwmt8"}},{"cell_type":"markdown","source":["- He 초기화를 활용한 backward pass"],"metadata":{"id":"QQmK9qoN1Xjf"}},{"cell_type":"code","source":["K = 50\n","D = 80\n","\n","def re_init_params(K, D):\n","\n","  np.random.seed(0)\n","\n","  D_i = 1\n","  D_o = 1\n","\n","  all_weights = [None] * (K+1)\n","  all_biases = [None] * (K+1)\n","\n","  all_weights[0] = np.random.normal(size=(D, D_i)) * np.sqrt(2 / D)\n","  all_weights[-1] = np.random.normal(size=(D_o, D)) * np.sqrt(2 / D_o)\n","  all_biases[0] = np.zeros((D,1))\n","  all_biases[-1]= np.zeros((D_o,1))\n","\n","  for layer in range(1,K):\n","    all_weights[layer] = np.random.normal(size=(D,D)) * np.sqrt(2 / D)\n","    all_biases[layer] = np.zeros((D,1))\n","\n","  return all_weights, all_biases"],"metadata":{"id":"kzGNKdLl_nLt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_weights, all_biases = re_init_params(K,D)\n","\n","# 단순화를 위해 첫 번째 은닉층과 마지막 은닉층 사이의 가중치와 편향의 기울기만 고려\n","n_data = 100 # 데이터 포인트 수\n","aggregate_dl_df = [None] * (K+1) # 뭐하는 사람이야\n","for layer in range(1,K): # 1~K(4) ->  은닉층\n","  # 3D array는 모든 데이터 포인트에 대한 그래디언트를 저장\n","  aggregate_dl_df[layer] = np.zeros((D,n_data)) # 층별로 (8, 100) 만큼 공간 확보 -> 은닉뉴런수가 8개고 데이터 포인터가 100개 이기 때문\n","\n","\n","# 각 데이터 포인트에 대한 매개 변수의 도함수를 개별적으로 계산해야함\n","for c_data in range(n_data):\n","  data_in = np.random.normal(size=(1,1))\n","  y = np.zeros((1,1))\n","  net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)\n","  all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df = backward_pass(all_weights, all_biases, all_f, all_h, y)\n","  for layer in range(1,K): # 1~K(4) ->  은닉층\n","    aggregate_dl_df[layer][:,c_data] = np.squeeze(all_dl_df[layer])\n","\n","for layer in range(1,K):\n","  print(\"Layer %d, std of dl_dh = %3.3f\"%(layer, np.std(aggregate_dl_df[layer].ravel())))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCLt7T9XBLEn","outputId":"686bd309-2ae7-481e-d561-5714a368b102"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer 1, std of dl_dh = 2.439\n","Layer 2, std of dl_dh = 2.572\n","Layer 3, std of dl_dh = 2.935\n","Layer 4, std of dl_dh = 2.876\n","Layer 5, std of dl_dh = 2.683\n","Layer 6, std of dl_dh = 2.791\n","Layer 7, std of dl_dh = 2.920\n","Layer 8, std of dl_dh = 2.613\n","Layer 9, std of dl_dh = 2.622\n","Layer 10, std of dl_dh = 2.980\n","Layer 11, std of dl_dh = 3.040\n","Layer 12, std of dl_dh = 3.056\n","Layer 13, std of dl_dh = 2.792\n","Layer 14, std of dl_dh = 2.899\n","Layer 15, std of dl_dh = 3.037\n","Layer 16, std of dl_dh = 2.765\n","Layer 17, std of dl_dh = 2.364\n","Layer 18, std of dl_dh = 2.397\n","Layer 19, std of dl_dh = 2.148\n","Layer 20, std of dl_dh = 2.639\n","Layer 21, std of dl_dh = 2.426\n","Layer 22, std of dl_dh = 2.150\n","Layer 23, std of dl_dh = 2.224\n","Layer 24, std of dl_dh = 1.954\n","Layer 25, std of dl_dh = 2.004\n","Layer 26, std of dl_dh = 2.793\n","Layer 27, std of dl_dh = 2.313\n","Layer 28, std of dl_dh = 2.563\n","Layer 29, std of dl_dh = 2.771\n","Layer 30, std of dl_dh = 2.996\n","Layer 31, std of dl_dh = 3.072\n","Layer 32, std of dl_dh = 2.969\n","Layer 33, std of dl_dh = 3.140\n","Layer 34, std of dl_dh = 3.535\n","Layer 35, std of dl_dh = 3.294\n","Layer 36, std of dl_dh = 3.337\n","Layer 37, std of dl_dh = 2.939\n","Layer 38, std of dl_dh = 2.834\n","Layer 39, std of dl_dh = 2.535\n","Layer 40, std of dl_dh = 2.833\n","Layer 41, std of dl_dh = 2.807\n","Layer 42, std of dl_dh = 3.385\n","Layer 43, std of dl_dh = 3.709\n","Layer 44, std of dl_dh = 4.064\n","Layer 45, std of dl_dh = 4.385\n","Layer 46, std of dl_dh = 4.204\n","Layer 47, std of dl_dh = 4.290\n","Layer 48, std of dl_dh = 3.935\n","Layer 49, std of dl_dh = 3.343\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}