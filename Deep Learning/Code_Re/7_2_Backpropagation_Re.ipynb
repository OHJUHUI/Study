{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap07/7_2_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"L6chybAVFJW2"},"source":["# **Notebook 7.2: Backpropagation**\n","\n","This notebook runs the backpropagation algorithm on a deep neural network as described in section 7.4 of the book.\n","\n","Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n","\n","Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LdIDglk1FFcG"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["\\"],"metadata":{"id":"uRAgO2-DpDK2"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"4hGY4pMzpDNM"}},{"cell_type":"markdown","metadata":{"id":"nnUoI0m6GyjC"},"source":["### 1. 신경망 구현"]},{"cell_type":"markdown","metadata":{"id":"z7NgM2COSxaw"},"source":["- 정의된 신경망 구조에 따라 초기 파라피터 값을 랜덤으로 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVM4Tc_jGI0Q"},"outputs":[],"source":["# 랜덤 시드 생성, 난수 추출 시 고정된 값들이 출력됨\n","np.random.seed(0)  # 0이상의 정수\n","\n","K = 5  # 은닉층\n","D = 6  # 은닉뉴런\n","D_i = 1  # 입력층\n","D_o = 1  # 출력층\n","\n","# 파라미터 초기화\n","all_weights = [None] * (K+1)  # [None, None, None, None, None, None] - 길이가 6인 리스트\n","all_biases = [None] * (K+1)  # [None, None, None, None, None, None]\n","# 리스트의 길이는 신경망의 레이어 수(K)에 따라 결정\n","\n","# 입력층 및 출력층\n","all_weights[0] = np.random.normal(size=(D, D_i))  # 입력층\n","all_weights[-1] = np.random.normal(size=(D_o, D))  # 출력층\n","all_biases[0] = np.random.normal(size =(D,1))  # 입력층\n","all_biases[-1]= np.random.normal(size =(D_o,1))  # 출력층\n","\n","# 중간 층 생성\n","for layer in range(1,K):\n","  all_weights[layer] = np.random.normal(size=(D,D))\n","  all_biases[layer] = np.random.normal(size=(D,1))"]},{"cell_type":"markdown","source":["**배열(Array)** <br>\n","배열의 크기는 고정되어 있으며, 선언 시에 크기를 지정해야함 <br><br>\n","**리스트(List)** <br>\n","원소의 개수가 가변적이며, 삽입과 삭제가 자유롭다. <br><br>\n","\n","->  다양한 신경망 구조를 다루기 위해서는 각 신경망에 따라 필요한 파라미터의 크기가 다르기 때문에 <br>\n","      리스트를 사용하여 미리 공간을 할당하고, 필요한 시점에 배열을 생성하여      해당 위치에 저장"],"metadata":{"id":"fKm6zakS8p_F"}},{"cell_type":"markdown","source":["\\"],"metadata":{"id":"_m15y8Y0qhW5"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"JgfwHv_1qhZW"}},{"cell_type":"markdown","source":["### 2. Forward pass"],"metadata":{"id":"i56hI_9cqks_"}},{"cell_type":"markdown","source":["- ReLU Function 정의"],"metadata":{"id":"q2seye3SIzq6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZh-7bPXIDq4"},"outputs":[],"source":["# ReLU function 정의\n","def ReLU(preactivation):\n","  activation = preactivation.clip(0.0)\n","  return activation"]},{"cell_type":"markdown","metadata":{"id":"5irtyxnLJSGX"},"source":["- 신경망의 순방향(forward pass) 계산"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgquJUJvJPaN"},"outputs":[],"source":["def compute_network_output(net_input, all_weights, all_biases):\n","\n","  K = len(all_weights) -1   # 5\n","\n","  # all_f: 각 층의 사전 활성화 함수\n","  # all_h: 각 층의 활성화 함수\n","  all_f = [None] * (K+1)\n","  all_h = [None] * (K+1)\n","\n","  # 입력층\n","  all_h[0] = net_input\n","\n","  # 중간층\n","  for layer in range(K):  # 0~4\n","      all_f[layer] = all_biases[layer] + np.matmul(all_weights[layer], all_h[layer])\n","      all_h[layer+1] = ReLU(all_f[layer])\n","\n","  # 출력층\n","  all_f[K] = all_biases[K] + np.matmul(all_weights[K], all_h[K])\n","  net_output = all_f[K]\n","\n","  return net_output, all_f, all_h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IN6w5m2ZOhnB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d6181179-8552-438f-9355-ba7653f5cc4d","executionInfo":{"status":"ok","timestamp":1717047999822,"user_tz":-540,"elapsed":16,"user":{"displayName":"문수연","userId":"11328078424481942837"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["True output = 1.907, Your answer = 1.907\n"]}],"source":["# check\n","\n","net_input = np.ones((D_i,1)) * 1.2\n","net_output, all_f, all_h = compute_network_output(net_input,all_weights, all_biases)\n","print(\"True output = %3.3f, Your answer = %3.3f\"%(1.907, net_output[0,0]))"]},{"cell_type":"markdown","source":["\\"],"metadata":{"id":"_TfzDXkHsLcy"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"XBYFbLFqsLe2"}},{"cell_type":"markdown","metadata":{"id":"JqtbrBZ-Sxa2"},"source":["### 3. Backward pass"]},{"cell_type":"markdown","metadata":{"id":"SxVTKp3IcoBF"},"source":["- 손실 함수 정의 및 출력에 대한 손실함수의 도함수 계산"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XqWSYWJdhQR"},"outputs":[],"source":["def least_squares_loss(net_output, y):\n","  return np.sum((net_output-y) * (net_output-y))\n","\n","def d_loss_d_output(net_output, y):\n","    return 2*(net_output -y);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njF2DUQmfttR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"02efc507-2011-47ca-e21c-66b44738d6c2","executionInfo":{"status":"ok","timestamp":1717047999822,"user_tz":-540,"elapsed":13,"user":{"displayName":"문수연","userId":"11328078424481942837"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["y = 20.000 Loss = 327.371\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-811c7c3c79c2>:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n","  print(\"y = %3.3f Loss = %3.3f\"%(y, loss))\n"]}],"source":["y = np.ones((D_o,1)) * 20.0\n","loss = least_squares_loss(net_output, y)\n","print(\"y = %3.3f Loss = %3.3f\"%(y, loss))"]},{"cell_type":"markdown","metadata":{"id":"98WmyqFYWA-0"},"source":["- 신경망의 backward pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJng7WpRPLMz"},"outputs":[],"source":["# indicator function\n","def indicator_function(x):\n","  x_in = np.array(x)\n","  x_in[x_in>=0] = 1\n","  x_in[x_in<0] = 0\n","  return x_in\n","\n","def backward_pass(all_weights, all_biases, all_f, all_h, y):\n","  # 편향, 가중치, f, h에 대한 손실함수의 도함수 저장\n","  all_dl_dweights = [None] * (K+1)\n","  all_dl_dbiases = [None] * (K+1)\n","  all_dl_df = [None] * (K+1)\n","  all_dl_dh = [None] * (K+1)\n","\n","  # 출력에 대한 손실의 도함수 계산\n","  all_dl_df[K] = np.array(d_loss_d_output(all_f[K],y))\n","\n","  # 역방향으로 작동\n","  for layer in range(K,-1,-1):  # k(5)~0 (역방향으로 작동)\n","    all_dl_dbiases[layer] = all_dl_df[layer]\n","    all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].T)\n","    all_dl_dh[layer] = np.matmul(all_weights[layer].T, all_dl_df[layer])\n","\n","    if layer > 0:\n","      all_dl_df[layer-1] = all_dl_dh[layer] * indicator_function(all_f[layer-1])\n","\n","  return all_dl_dweights, all_dl_dbiases"]},{"cell_type":"code","source":["all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)"],"metadata":{"id":"ufX6krium27J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\\"],"metadata":{"id":"7op6N97Hu7QO"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"4TtZxMGQu7nW"}},{"cell_type":"markdown","source":["### 4. 유한차분(finite difference)을 사용한 역전파 구현의 정확성 검증"],"metadata":{"id":"IVNNWrAku764"}},{"cell_type":"markdown","source":["<br>\n","$\n","f'(x) ≈  \\frac{f(x+ϵ) - f(x)}{ϵ}\n","$\n","<br><br>"],"metadata":{"id":"RXhPJ0HMvJrc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PK-UtE3hreAK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"de05dac7-5f46-479f-9387-58064e7a19bb","executionInfo":{"status":"ok","timestamp":1717048000286,"user_tz":-540,"elapsed":474,"user":{"displayName":"문수연","userId":"11328078424481942837"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------\n","Bias 0, derivatives from backprop:\n","[[ -4.486]\n"," [  4.947]\n"," [  6.812]\n"," [ -3.883]\n"," [-24.935]\n"," [  0.   ]]\n","Bias 0, derivatives from finite differences\n","[[ -4.486]\n"," [  4.947]\n"," [  6.812]\n"," [ -3.883]\n"," [-24.935]\n"," [  0.   ]]\n","Success!  Derivatives match.\n","-----------------------------------------------\n","Bias 1, derivatives from backprop:\n","[[ -0.   ]\n"," [-11.297]\n"," [  0.   ]\n"," [  0.   ]\n"," [-10.722]\n"," [  0.   ]]\n","Bias 1, derivatives from finite differences\n","[[  0.   ]\n"," [-11.297]\n"," [  0.   ]\n"," [  0.   ]\n"," [-10.722]\n"," [  0.   ]]\n","Success!  Derivatives match.\n","-----------------------------------------------\n","Bias 2, derivatives from backprop:\n","[[-0.   ]\n"," [-0.   ]\n"," [ 0.938]\n"," [ 0.   ]\n"," [-9.993]\n"," [ 0.508]]\n","Bias 2, derivatives from finite differences\n","[[ 0.   ]\n"," [ 0.   ]\n"," [ 0.938]\n"," [ 0.   ]\n"," [-9.993]\n"," [ 0.508]]\n","Success!  Derivatives match.\n","-----------------------------------------------\n","Bias 3, derivatives from backprop:\n","[[-0.   ]\n"," [-4.8  ]\n"," [-1.661]\n"," [-0.   ]\n"," [ 3.393]\n"," [ 5.391]]\n","Bias 3, derivatives from finite differences\n","[[ 0.   ]\n"," [-4.8  ]\n"," [-1.661]\n"," [ 0.   ]\n"," [ 3.393]\n"," [ 5.391]]\n","Success!  Derivatives match.\n","-----------------------------------------------\n","Bias 4, derivatives from backprop:\n","[[-0.   ]\n"," [ 0.   ]\n"," [ 0.   ]\n"," [-0.   ]\n"," [-5.212]\n"," [-0.   ]]\n","Bias 4, derivatives from finite differences\n","[[ 0.   ]\n"," [ 0.   ]\n"," [ 0.   ]\n"," [ 0.   ]\n"," [-5.212]\n"," [ 0.   ]]\n","Success!  Derivatives match.\n","-----------------------------------------------\n","Weight 0, derivatives from backprop:\n","[[ -5.383]\n"," [  5.937]\n"," [  8.174]\n"," [ -4.66 ]\n"," [-29.922]\n"," [  0.   ]]\n","Weight 0, derivatives from finite differences\n","[[ -5.383]\n"," [  5.937]\n"," [  8.174]\n"," [ -4.66 ]\n"," [-29.922]\n"," [  0.   ]]\n","Success!  Derivatives match.\n","-----------------------------------------------\n","Weight 1, derivatives from backprop:\n","[[  0.      0.      0.      0.      0.      0.   ]\n"," [-32.511  -6.799 -18.282 -34.148 -42.196   0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [-30.856  -6.453 -17.352 -32.409 -40.047   0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]]\n","Weight 1, derivatives from finite differences\n","[[  0.      0.      0.      0.      0.      0.   ]\n"," [-32.511  -6.799 -18.282 -34.148 -42.196   0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [-30.856  -6.453 -17.352 -32.409 -40.047   0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]]\n","Success!  Derivatives match.\n","-----------------------------------------------\n","Weight 2, derivatives from backprop:\n","[[  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      5.371   0.      0.      3.145   0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.    -57.233   0.      0.    -33.506   0.   ]\n"," [  0.      2.907   0.      0.      1.702   0.   ]]\n","Weight 2, derivatives from finite differences\n","[[  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      5.371   0.      0.      3.145   0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.    -57.233   0.      0.    -33.506   0.   ]\n"," [  0.      2.907   0.      0.      1.702   0.   ]]\n","Success!  Derivatives match.\n","-----------------------------------------------\n","Weight 3, derivatives from backprop:\n","[[  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.     -3.674   0.    -42.905 -10.998]\n"," [  0.      0.     -1.272   0.    -14.85   -3.807]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      2.597   0.     30.333   7.776]\n"," [  0.      0.      4.126   0.     48.188  12.353]]\n","Weight 3, derivatives from finite differences\n","[[  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.     -3.674   0.    -42.905 -10.998]\n"," [  0.      0.     -1.272   0.    -14.85   -3.807]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      2.597   0.     30.333   7.776]\n"," [  0.      0.      4.126   0.     48.188  12.353]]\n","Success!  Derivatives match.\n","-----------------------------------------------\n","Weight 4, derivatives from backprop:\n","[[  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.    -81.635 -49.136   0.    -22.007 -10.146]\n"," [  0.      0.      0.      0.      0.      0.   ]]\n","Weight 4, derivatives from finite differences\n","[[  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.    -81.635 -49.136   0.    -22.007 -10.146]\n"," [  0.      0.      0.      0.      0.      0.   ]]\n","Success!  Derivatives match.\n"]}],"source":["# 출력 설정\n","np.set_printoptions(precision=3)  # 출력 시 소수점은 셋째자리 까지만 출력함.\n","\n","# 유한차분법으로 계산된 파라미터 값 저장\n","all_dl_dweights_fd = [None] * (K+1)\n","all_dl_dbiases_fd = [None] * (K+1)\n","\n","# 미세변화량 설정\n","delta_fd = 0.000001\n","\n","# 편향의 기울기 검정\n","for layer in range(K):\n","\n","  # 현재 층의 편향에 대한 손실함수의 도함수 저장 공간 설정\n","  dl_dbias  = np.zeros_like(all_dl_dbiases[layer]) # 현재 층의 바이어스의 기울기 배열\n","\n","  for row in range(all_biases[layer].shape[0]): # 현재 층의 바이어스의 행 개수(길이) ex) all_dbiases[0].shape[0] -> 6\n","\n","    # 편향의 변화\n","    all_biases_copy = [np.array(x) for x in all_biases] # 모든 바이어스 배열의 복사본 -> 기존배열을 변경하지 않기 위해\n","    all_biases_copy[layer][row] += delta_fd\n","\n","    # 신경망 계산\n","    network_output_1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)\n","    network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n","\n","    # 유한차분법을 이용한 도함수 계산\n","    dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n","\n","  all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n","  print(\"-----------------------------------------------\")\n","  print(\"Bias %d, derivatives from backprop:\"%(layer))\n","  print(all_dl_dbiases[layer])\n","  print(\"Bias %d, derivatives from finite differences\"%(layer))\n","  print(all_dl_dbiases_fd[layer])\n","\n","  if np.allclose(all_dl_dbiases_fd[layer],all_dl_dbiases[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n","    # equal_nan:  NaN 값이 있는 경우에도 두 배열이 완전히 같아야 함.\n","    print(\"Success!  Derivatives match.\")\n","  else:\n","    print(\"Failure!  Derivatives different.\")\n","\n","\n","\n","# 가중치의 기울기 검정\n","for layer in range(K):\n","  dl_dweight  = np.zeros_like(all_dl_dweights[layer])\n","  # For every element in the bias\n","  for row in range(all_weights[layer].shape[0]):\n","    for col in range(all_weights[layer].shape[1]):\n","      # Take copy of biases  We'll change one element each time\n","      all_weights_copy = [np.array(x) for x in all_weights]\n","      all_weights_copy[layer][row][col] += delta_fd\n","      network_output_1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)\n","      network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n","      dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n","  all_dl_dweights_fd[layer] = np.array(dl_dweight)\n","  print(\"-----------------------------------------------\")\n","  print(\"Weight %d, derivatives from backprop:\"%(layer))\n","  print(all_dl_dweights[layer])\n","  print(\"Weight %d, derivatives from finite differences\"%(layer))\n","  print(all_dl_dweights_fd[layer])\n","  if np.allclose(all_dl_dweights_fd[layer],all_dl_dweights[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n","    print(\"Success!  Derivatives match.\")\n","  else:\n","    print(\"Failure!  Derivatives different.\")"]},{"cell_type":"markdown","source":["\n","```\n","np.allclose(a, b, rtol=1e-05, atol=1e-08, equal_nan=False)\n","```\n","\n","> a : 첫 번째 입력 배열. <br>\n","b : 두 번째 입력 배열.<br>\n","rtol : 상대 허용 오차 (relative tolerance). 기본값은 1e-05.<br>\n","atol : 절대 허용 오차 (absolute tolerance). 기본값은 1e-08.<br>\n","equal_nan : True로 설정하면 NaN 값들도 서로 같다고 간주함. 기본값은 False.\n","\n","\n"," 두 배열의 요소들이 모두 주어진 허용 오차 범위 내에서 가까운지 여부를 확인하는 데 사용하는 함수이며, 두 배열의 모든 요소가 허용 오차 내에서 가까우면 True, 그렇지 않으면 False를 반환함.\n","\n","   "],"metadata":{"id":"3vzFXAX-cxXH"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}